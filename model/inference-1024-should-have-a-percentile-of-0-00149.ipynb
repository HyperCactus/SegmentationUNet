{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### Inference: \n","From https://www.kaggle.com/code/misakimatsutomo/inference-1024-should-have-a-percentile-of-0-00149/notebook\n","\n","Adapting to work with my segmentation model\n","\n","model:\n","Improved attention gated UNet\n","\n","idia 0:        \n","         th_percentile = 0.00149                            (-> 0.857)\n","\n","\n","# Train: \n","https://www.kaggle.com/nguynvnthtr/the-training-image-is-1024-x-1024  \n","\n","version 1\n","\n","idia  : image_size = 1024\n","\n","idia 1:\n","\n","        x_index= (x.shape[1]-self.image_size)//2 \n","        \n","        y_index= (x.shape[2]-self.image_size)//2   (-> 0.628)\n","        \n","idia 2:\n","     \n","        p_augm = 0.05  #probability of augmentaton changed to 0.05    (-> 0.686)\n","        \n","\n","    \n","idia 3:\n","\n","        in_chans = 1  #window for moov label changed to 1    (-> 0.800)\n","        \n","idia 4:\n"," \n","        Epoch = 27                                           (-> 0.856)\n","\n","\n","idia 5:\n","\n","        lr = 8e-5"]},{"cell_type":"markdown","metadata":{},"source":["# All get from:"]},{"cell_type":"markdown","metadata":{},"source":["**This code is base on [2.5d segmentaion baseline [inference]](https://www.kaggle.com/code/tanakar/2-5d-segmentaion-baseline-inference)**\n","If you think my code is useful,please upvote it ^w^.\n","* Version2:\n","1. *     updata normalization method\n","2. *     image_size = 512\n","3. *     useing 3d TTA\n","4. *     se_resnext50_32x4d\n","\n","* Version3:\n","1. *     updata normalization method\n","\n","* This version is correspond with [2.5d segmentaion baseline [training]](https://www.kaggle.com/code/yoyobar/2-5d-cutting-model-baseline-training) version6\n"]},{"cell_type":"markdown","metadata":{},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-12T11:41:46.129151Z","iopub.status.busy":"2024-01-12T11:41:46.128501Z","iopub.status.idle":"2024-01-12T11:42:12.150702Z","shell.execute_reply":"2024-01-12T11:42:12.149845Z","shell.execute_reply.started":"2024-01-12T11:41:46.129116Z"},"trusted":true},"outputs":[],"source":["import torch as tc \n","import torch.nn as nn  \n","import numpy as np\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","import cv2\n","import os,sys\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","# !python -m pip install --no-index --find-links=/kaggle/input/pip-download-for-segmentation-models-pytorch segmentation-models-pytorch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.parallel import DataParallel\n","# from dotenv import load_dotenv\n"]},{"cell_type":"markdown","metadata":{},"source":["# config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T11:42:12.152744Z","iopub.status.busy":"2024-01-12T11:42:12.152450Z","iopub.status.idle":"2024-01-12T11:42:12.160220Z","shell.execute_reply":"2024-01-12T11:42:12.159243Z","shell.execute_reply.started":"2024-01-12T11:42:12.152719Z"},"trusted":true},"outputs":[],"source":["model_path_i = 9 # 7 #5 #in_chans_1__25     3 in_chans_1__20 2 \n","class CFG:\n","# ============== model CFG =============\n","    model_name = 'Unet'\n","    backbone = 'se_resnext50_32x4d'\n","\n","    in_chans = 3#1 #5 # 65\n","    #============== _ CFG =============\n","    image_size = 512#1024\n","    input_size= 512#1024\n","    tile_size = image_size\n","    stride = tile_size // 4\n","    drop_egde_pixel= 0 # 16 #32\n","    \n","    target_size = 1\n","    chopping_percentile=1e-3\n","    # ============== fold =============\n","    valid_id = 1\n","    batch=4#16 #128\n","    th_percentile = 0.00143 #0.00145 #0.00146 #0.00149 #0.00145 # 0.0014 #0.00175 #0.0021\n","    \n","    path_submition = 0 # public / private\n","    #axis_w = [0.3353333 ,0.3323333,0.3323333 ]\n","    model_path=[\"/kaggle/input/2-5d-cutting-model-baseline-training/se_resnext50_32x4d_19_loss0.12_score0.79_val_loss0.25_val_score0.79.pt\",\n","               \"/kaggle/input/training-6-512/se_resnext50_32x4d_19_loss0.09_score0.83_val_loss0.28_val_score0.83.pt\",\n","               \"/kaggle/input/training-6-512/se_resnext50_32x4d_19_loss0.05_score0.90_val_loss0.25_val_score0.86.pt\",\n","               \"/kaggle/input/training-6-512/se_resnext50_32x4d_19_loss0.05_score0.89_val_loss0.24_val_score0.86_midd.pt\",\n","               \"/kaggle/input/training-6-512/se_resnext50_32x4d_24_loss0.05_score0.90_val_loss0.23_val_score0.88_midd.pt\",\n","               \"/kaggle/input/training-6-512/se_resnext50_32x4d_24_loss0.04_score0.91_val_loss0.23_val_score0.88_midd.pt\", # 25 025 rot 512 center\n","               \"/kaggle/input/blood-vessel-model-1024/se_resnext50_32x4d_24_loss0.10_score0.90_val_loss0.16_val_score0.85_midd_1024.pt\",\n","               \"/kaggle/input/blood-vessel-model-1024/se_resnext50_32x4d_24_loss0.10_score0.90_val_loss0.12_val_score0.88_midd_1024.pt\",# lr = 8e-5\n","               \"/kaggle/input/blood-vessel-model-1024/se_resnext50_32x4d_24_loss0.91_score0.09_val_loss0.91_val_score0.09_midd_1024.pt\", #  60e-5 \n","               \"/kaggle/input/sn-hoa-8e-5-27-rot0-5/se_resnext50_32x4d_26_loss0.10_score0.90_val_loss0.12_val_score0.88_midd_1024.pt\",#8e-5-27-rot0-5\n","               \"/kaggle/input/sn-hoa-8e-5-27-rot0-5/se_resnext50_32x4d_30_loss0.10_score0.90_val_loss0.13_val_score0.88_midd_1024.pt\"]#  31 8e 05  \n","    \n","    # checkpoint_dir = '/kaggle/input/checkpoint/checkpoints/checkpoint.pth.tar'\n","    checkpoint_dir = '../../checkpoints/checkpoint.pth.tar'"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T11:42:12.161821Z","iopub.status.busy":"2024-01-12T11:42:12.161580Z","iopub.status.idle":"2024-01-12T11:42:12.206304Z","shell.execute_reply":"2024-01-12T11:42:12.205452Z","shell.execute_reply.started":"2024-01-12T11:42:12.161800Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","A CNN model based on the Improved UNet architecture, with associated modules.\n","\"\"\"\n","# This is my 2D UNet from COMP3710\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","\n","class ContextModule(nn.Module):\n","    \"\"\"\n","    This is the context module from the improved UNet architecture.\n","    \"Each context module is in fact a pre-activation residual block with two\n","    3x3 convolutional layers and a dropout layer (pdrop = 0.3) in between.\"\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Initialize the ContextModule.\n","        \"\"\"\n","        super(ContextModule, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.norm1 = nn.InstanceNorm2d(out_channels)\n","        self.relu1 = nn.LeakyReLU(negative_slope=1e-2)\n","        self.dropout = nn.Dropout2d(p=0.3)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.norm2 = nn.InstanceNorm2d(out_channels)\n","        self.relu2 = nn.LeakyReLU(negative_slope=1e-2)\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the ContextModule.\n","        \"\"\"\n","        out = self.conv1(x)\n","        out = self.norm1(out)\n","        out = self.relu1(out)\n","        out = self.dropout(out)\n","        out = self.conv2(out)\n","        out = self.norm2(out)\n","        out = self.relu2(out)\n","        out += x # residual connection\n","        return out\n","\n","class LocalisationModule(nn.Module):\n","    \"\"\"\n","    This is the localization module from the improved UNet architecture.\n","    A localization module consists of a 3x3 convolution followed by a 1x1 convolution that \n","    halves the number of feature maps.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Initialize the LocalisationModule.\n","        \"\"\"\n","        super(LocalisationModule, self).__init__()\n","        # self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n","        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","    \n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the LocalisationModule.\n","        \"\"\"\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        return out\n","\n","\n","class UpsamplingModule(nn.Module):\n","    \"\"\"\n","    An upsampling module consists of an upsampling layer that repeats the feature pixels \n","    twice in each spatial dimension followed by a 3x3 convolution.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Initialize the UpsamplingModule.\n","        \"\"\"\n","        super(UpsamplingModule, self).__init__()\n","        self.layers = nn.Sequential(\n","            nn.Upsample(scale_factor=2, mode='nearest'),\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        )\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the UpsamplingModule.\n","        \"\"\"\n","        out = self.layers(x)\n","        return out\n","\n","class AttentionBlock(nn.Module):\n","    \"\"\"Attention block with learnable parameters\"\"\"\n","\n","    def __init__(self, F_g, F_l, n_coefficients):\n","        \"\"\"\n","        :param F_g: number of feature maps (channels) in previous layer\n","        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n","        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n","        \"\"\"\n","        super(AttentionBlock, self).__init__()\n","\n","        self.W_gate = nn.Sequential(\n","            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(n_coefficients)\n","        )\n","\n","        self.W_x = nn.Sequential(\n","            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(n_coefficients)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, gate, skip_connection):\n","        \"\"\"\n","        :param gate: gating signal from previous layer\n","        :param skip_connection: activation from corresponding encoder layer\n","        :return: output activations\n","        \"\"\"\n","        g1 = self.W_gate(gate)\n","        x1 = self.W_x(skip_connection)\n","        psi = self.relu(g1 + x1)\n","        psi = self.psi(psi)\n","        out = skip_connection * psi\n","        return out\n","\n","\n","class ImprovedUNet(nn.Module):\n","    \"\"\"\n","    This is the improved UNet model, it consists of the context aggregation pathway (encoder)\n","    and the localization pathway (decoder). The model is designed to outpreform the original\n","    UNet for medical image segmentation tasks.\n","    See https://arxiv.org/pdf/1802.10508v1.pdf for network architecture.\n","    \"\"\"\n","    def __init__(self, CFG, in_channels=3, out_channels=1, features=[16,32,64,128,256]):\n","        \"\"\"\n","        Initialize the ImprovedUNet model by creating all necessary layers.\n","\n","        Args:\n","            in_channels (int, optional): Number of input channels. Defaults to 3 for RGB color images.\n","            out_channels (int, optional): Number of output channels. Defaults to 1 a for greyscale binary \n","            segmentation mask.\n","            features (list, optional): The numbers of feature maps to extract (must be length 5 and each \n","            entry must be double the previous entry). Defaults to [16,32,64,128,256].\n","        \"\"\"\n","        super(ImprovedUNet, self).__init__()\n","        self.CFG = CFG\n","        self.encoder_block1 = nn.Sequential(\n","            nn.Conv2d(in_channels, features[0], kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[0], features[0]), # 3 channels in, 16 channels out\n","        )\n","        self.encoder_block2 = nn.Sequential(\n","            nn.Conv2d(features[0], features[1], kernel_size=3, stride=2, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[1], features[1]), # 16 channels in, 32 channels out\n","        )\n","        self.encoder_block3 = nn.Sequential(\n","            nn.Conv2d(features[1], features[2], kernel_size=3, stride=2, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[2], features[2]), # 32 channels in, 64 channels out\n","        )\n","        self.encoder_block4 = nn.Sequential(\n","            nn.Conv2d(features[2], features[3], kernel_size=3, stride=2, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[3], features[3]), # 64 channels in, 128 channels out\n","        )\n","        self.encoder_block5 = nn.Sequential(\n","            nn.Conv2d(features[3], features[4], kernel_size=3, stride=2, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[4], features[4]), # 128 channels in, 256 channels out\n","            # upsampling module in last encoder block increases spatial dimensions by 2 for decoder\n","            UpsamplingModule(features[4], features[3]), # 256 channels in, 128 channels out\n","        )\n","        \n","        # Adding in attention blocks gates at the concatenation of the skip connection and the bottleneck layer\n","        self.attention1 = AttentionBlock(features[3], features[3], 32)\n","        \n","        # Upsampling modules half the number of feature maps but after upsampling, the output\n","        # is concatenated with the skip connection, so the number of feature maps is doubled\n","        # the localization modules then halve the number of feature maps again\n","        self.decoder_block1 = nn.Sequential(\n","            LocalisationModule(features[4], features[3]), # 256 channels in, 128 channels out\n","            UpsamplingModule(features[3], features[2]), # 128 channels in, 64 channels out\n","        )\n","        self.attention2 = AttentionBlock(features[2], features[2], 64)\n","        # these decoder layers need to be split up to allow for skip connections\n","        self.localisation2 = LocalisationModule(features[3], features[2]) # 128 channels in, 64 channels out\n","        self.up3 = UpsamplingModule(features[2], features[1]) # 64 channels in, 32 channels out, double spatial dimensions\n","        self.attention3 = AttentionBlock(features[1], features[1], 128)\n","        self.localisation3 = LocalisationModule(features[2], features[1]) # 64 channels in, 32 channels out\n","        self.up4 = UpsamplingModule(features[1], features[0]) # 32 channels in, 16 channels out, double spatial dimensions\n","        self.attention4 = AttentionBlock(features[0], features[0], 256)\n","        self.final_conv = nn.Sequential(\n","            nn.Conv2d(features[1], features[1], kernel_size=1), # 32 channels in, 32 channels out, final convolutional layer\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            nn.Conv2d(features[1], out_channels, kernel_size=1) # 32 channels in, 1 channel out, segmentation layer\n","        )\n","        \n","        self.segmentation1 = nn.Conv2d(features[2], out_channels, kernel_size=1) # 64 channels in, 1 channels out, segmentation layer\n","        self.upscale1 = nn.Upsample(scale_factor=2, mode='nearest') # upscale the segmentation layer to match the dimensions of the output\n","        self.segmentation2 = nn.Conv2d(features[1], out_channels, kernel_size=1) # 32 channels in, 1 channels out, segmentation layer\n","        self.upscale2 = nn.Upsample(scale_factor=2, mode='nearest') # upscale the segmentation layer to match the dimensions of the output\n","        \n","    \n","    def forward(self, x):\n","        \"\"\"Forward pass of the ImprovedUNet model to generate a binary segmentation mask.\n","\n","        Args:\n","            x (torch.Tensor): An input image in tensor form.\n","\n","        Returns:\n","            torch.Tensor: Binary segmentation mask in tensor form\n","        \"\"\"\n","        skip_connections = []\n","        \n","        x = self.encoder_block1(x) # 3 channels in, 16 channels out\n","        skip_connections.append(x)\n","        x = self.encoder_block2(x) # 16 channels in, 32 channels out\n","        skip_connections.append(x)\n","        x = self.encoder_block3(x) # 32 channels in, 64 channels out\n","        skip_connections.append(x)\n","        x = self.encoder_block4(x) # 64 channels in, 128 channels out\n","        skip_connections.append(x)\n","        # bottleneck layer\n","        x = self.encoder_block5(x) # 128 channels in, 128 channels out\n","        \n","        # use skip connections as a stack to allow for easy popping\n","        # concatenate the skip connection with 128 channels with the bottleneck layer with 128 channels\n","        # x = torch.cat((x, skip_connections.pop()), dim=1)\n","        \n","        # First attention block\n","        a = self.attention1(x, skip_connections.pop())\n","        x = torch.cat((a, x), dim=1)\n","        \n","        x = self.decoder_block1(x) # 256 channels in, 64 channels out\n","        # concatenate the skip connection with 64 channels with the bottleneck layer with 64 channels\n","        # x = torch.cat((x, skip_connections.pop()), dim=1)\n","        \n","        # Second attention block\n","        a = self.attention2(x, skip_connections.pop())\n","        x = torch.cat((a, x), dim=1)\n","        \n","        x = self.localisation2(x) # 128 channels in, 64 channels out\n","        # additional skip connections for segmentation layers in the decoder\n","        seg_connection1 = self.segmentation1(x) # 64 channels in, 1 channel out\n","        seg_connection1 = self.upscale1(seg_connection1)\n","        \n","        x = self.up3(x)\n","        \n","        # concatenate the skip connection with 32 channels with the bottleneck layer with 32 channels\n","        # x = torch.cat((x, skip_connections.pop()), dim=1)\n","        \n","        # Third attention block\n","        a = self.attention3(x, skip_connections.pop())\n","        x = torch.cat((a, x), dim=1)\n","        \n","        x = self.localisation3(x) # 64 channels in, 32 channels out\n","        # additional skip connections for segmentation layers in the decoder\n","        seg_connection2 = self.segmentation2(x) # 32 channels in, 1 channel out\n","        # element wise addition of the segmentation connections\n","        seg_connection2 += seg_connection1\n","        seg_connection2 = self.upscale2(seg_connection2) # upscale the segmentation connection to match the dimensions of the output\n","        \n","        x = self.up4(x)\n","        # concatenate the skip connection with 16 channels with the bottleneck layer with 16 channels\n","        # x = torch.cat((x, skip_connections.pop()), dim=1)\n","        \n","        # Fourth attention block\n","        a = self.attention4(x, skip_connections.pop())\n","        x = torch.cat((a, x), dim=1)\n","        \n","        x = self.final_conv(x) # 32 channels in, 1 channel out, includes final convolutional layer and segmentation layer\n","        \n","        # element wise addition of the segmentation connections\n","        x += seg_connection2\n","        \n","        # use sigmoid activation function to squash the output to between 0 and 1\n","        # x = torch.sigmoid(x)\n","#         return x\n","        # return x # no final activation function\n","    \n","        if CFG.input_size!=CFG.image_size:\n","            x=nn.functional.interpolate(x[None],size=(CFG.image_size,CFG.image_size),mode='bilinear',align_corners=True)[0]\n","        return x\n","\n","\n","\n","\n","\n","#========================================\n","\n","        \n","        \n","\n","\n","def build_model(weight=None):\n","    # load_dotenv()\n","    \n","    \n","    model = ImprovedUNet(CFG)\n","\n","    return model.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["# Size"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T11:42:12.208756Z","iopub.status.busy":"2024-01-12T11:42:12.208465Z","iopub.status.idle":"2024-01-12T11:42:12.221442Z","shell.execute_reply":"2024-01-12T11:42:12.220515Z","shell.execute_reply.started":"2024-01-12T11:42:12.208732Z"},"trusted":true},"outputs":[],"source":["def to_1024(img , image_size = 1024):\n","    if image_size > img.shape[1]:\n","       img = np.rot90(img)\n","       start1 = (CFG.image_size - img.shape[0])//2 \n","       top =     img[0                    : start1,   0: img.shape[1] ]\n","       bottom  = img[img.shape[0] -start1 : img.shape[0],   0 : img.shape[1] ]\n","       img_result = np.concatenate((top,img,bottom ),axis=0)\n","       img_result = np.rot90(img_result)\n","       img_result = np.rot90(img_result)\n","       img_result = np.rot90(img_result)\n","    else :\n","       img_result = img\n","    return img_result\n","\n","def to_1024_no_rot(img, image_size = 1024):\n","    if image_size > img.shape[0]:  \n","       start1 = ( image_size - img.shape[0])//2\n","       top =     img[0                    : start1,   0: img.shape[1] ]\n","       bottom  = img[img.shape[0] -start1 : img.shape[0],   0 : img.shape[1] ]\n","       img_result = np.concatenate((top,img,bottom ),axis=0)\n","    else: \n","       img_result = img\n","    return img_result\n","\n","def to_1024_1024(img  , image_size = 1024 ):\n","     img_result = to_1024(img, image_size )\n","     return img_result\n","    \n","def to_original ( im_after, img, image_size = 1024 ):\n","    top_ = 0\n","    left_ = 0\n","    if (im_after.shape[0] > img.shape[0]):\n","             top_  = ( image_size - img.shape[0])//2 \n","    if    (im_after.shape[1] > img.shape[1]) :\n","             left_  = ( image_size - img.shape[1])//2  \n","    if (top_>0)or (left_>0) :\n","             img_result = im_after[top_  : img.shape[0] + top_,   left_: img.shape[1] + left_ ]\n","    else:\n","             img_result = im_after\n","    return img_result  "]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T11:42:12.223236Z","iopub.status.busy":"2024-01-12T11:42:12.222896Z","iopub.status.idle":"2024-01-12T11:42:12.249650Z","shell.execute_reply":"2024-01-12T11:42:12.248757Z","shell.execute_reply.started":"2024-01-12T11:42:12.223212Z"},"trusted":true},"outputs":[],"source":["def rle_encode(mask):\n","    pixel = mask.flatten()\n","    pixel = np.concatenate([[0], pixel, [0]])\n","    run = np.where(pixel[1:] != pixel[:-1])[0] + 1\n","    run[1::2] -= run[::2]\n","    rle = ' '.join(str(r) for r in run)\n","    if rle == '':\n","        rle = '1 0'\n","    return rle\n","\n","def min_max_normalization(x:tc.Tensor)->tc.Tensor:\n","    \"\"\"input.shape=(batch,f1,...)\"\"\"\n","    shape=x.shape\n","    if x.ndim>2:\n","        x=x.reshape(x.shape[0],-1)\n","    \n","    min_=x.min(dim=-1,keepdim=True)[0]\n","    max_=x.max(dim=-1,keepdim=True)[0]\n","    if min_.mean()==0 and max_.mean()==1:\n","        return x.reshape(shape)\n","    \n","    x=(x-min_)/(max_-min_+1e-9)\n","    return x.reshape(shape)\n","\n","def norm_with_clip(x:tc.Tensor,smooth=1e-5):\n","    dim=list(range(1,x.ndim))\n","    mean=x.mean(dim=dim,keepdim=True)\n","    std=x.std(dim=dim,keepdim=True)\n","    x=(x-mean)/(std+smooth)\n","    x[x>5]=(x[x>5]-5)*1e-3 +5\n","    x[x<-3]=(x[x<-3]+3)*1e-3-3\n","    return x\n","\n","class Data_loader(Dataset):\n","    def __init__(self,path,s=\"/images/\"):\n","        self.paths=glob(path+f\"{s}*.tif\")\n","        self.paths.sort()\n","        self.bool=s==\"/labels/\"\n","    \n","    def __len__(self):\n","        return len(self.paths)\n","    \n","    def __getitem__(self,index):\n","        img=cv2.imread(self.paths[index],cv2.IMREAD_GRAYSCALE)\n","        img = to_1024_1024(img , image_size = CFG.image_size )\n","        \n","        img=tc.from_numpy(img.copy())\n","        if self.bool:\n","            img=img.to(tc.bool)\n","        else:\n","            img=img.to(tc.uint8)\n","        return img\n","\n","def load_data(path,s):\n","    data_loader=Data_loader(path,s)\n","    data_loader=DataLoader(data_loader, batch_size=16, num_workers=1)\n","    data=[]\n","    for x in tqdm(data_loader):\n","        data.append(x)\n","    x=tc.cat(data,dim=0)\n","    ########################################################################\n","    TH=x.reshape(-1).numpy()\n","    index = -int(len(TH) * CFG.chopping_percentile)\n","    TH:int = np.partition(TH, index)[index]\n","    x[x>TH]=int(TH)\n","    ########################################################################\n","    TH=x.reshape(-1).numpy()\n","    index = -int(len(TH) * CFG.chopping_percentile)\n","    TH:int = np.partition(TH, -index)[-index]\n","    x[x<TH]=int(TH)\n","    ########################################################################\n","    #x=(min_max_normalization(x.to(tc.float16))*255).to(tc.uint8)\n","    return x\n","\n","class Pipeline_Dataset(Dataset):\n","    def __init__(self,x,path):\n","        self.img_paths  = glob(path+\"/images/*\")\n","        self.img_paths.sort()\n","        self.in_chan = CFG.in_chans\n","        z=tc.zeros(self.in_chan//2,*x.shape[1:],dtype=x.dtype)\n","        self.x=tc.cat((z,x,z),dim=0)\n","        \n","    def __len__(self):\n","        return self.x.shape[0]-self.in_chan+1\n","    \n","    def __getitem__(self, index):\n","        x  = self.x[index:index+self.in_chan]\n","        return x,index\n","    \n","    def get_mark(self,index):\n","        id=self.img_paths[index].split(\"/\")[-3:]\n","        id.pop(1)\n","        id=\"_\".join(id)\n","        return id[:-4]\n","    \n","    def get_marks(self):\n","        ids=[]\n","        for index in range(len(self)):\n","            ids.append(self.get_mark(index))\n","        return ids\n","\n","def add_edge(x:tc.Tensor,edge:int):\n","    #x=(C,H,W)\n","    #output=(C,H+2*edge,W+2*edge)\n","    mean_=int(x.to(tc.float32).mean())\n","    x=tc.cat([x,tc.ones([x.shape[0],edge,x.shape[2]],dtype=x.dtype,device=x.device)*mean_],dim=1)\n","    x=tc.cat([x,tc.ones([x.shape[0],x.shape[1],edge],dtype=x.dtype,device=x.device)*mean_],dim=2)\n","    x=tc.cat([tc.ones([x.shape[0],edge,x.shape[2]],dtype=x.dtype,device=x.device)*mean_,x],dim=1)\n","    x=tc.cat([tc.ones([x.shape[0],x.shape[1],edge],dtype=x.dtype,device=x.device)*mean_,x],dim=2)\n","    return x\n","\n","def load_checkpoint(checkpoint, model, optimizer=None):\n","    \"\"\"\n","    Loads the model and optimizer state dicts from a checkpoint file\n","    \"\"\"\n","    print('>>> Loading checkpoint')\n","    model.load_state_dict(checkpoint['state_dict'])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","    print('>>> Checkpoint loaded')"]},{"cell_type":"markdown","metadata":{},"source":["# Build model(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T11:42:12.251138Z","iopub.status.busy":"2024-01-12T11:42:12.250829Z","iopub.status.idle":"2024-01-12T11:42:12.948866Z","shell.execute_reply":"2024-01-12T11:42:12.947977Z","shell.execute_reply.started":"2024-01-12T11:42:12.251114Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> Loading checkpoint\n",">>> Checkpoint loaded\n"]},{"data":{"text/plain":["ImprovedUNet(\n","  (encoder_block1): Sequential(\n","    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.01)\n","    (2): ContextModule(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu1): LeakyReLU(negative_slope=0.01)\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm2): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu2): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (encoder_block2): Sequential(\n","    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.01)\n","    (2): ContextModule(\n","      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu1): LeakyReLU(negative_slope=0.01)\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu2): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (encoder_block3): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.01)\n","    (2): ContextModule(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu1): LeakyReLU(negative_slope=0.01)\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu2): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (encoder_block4): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.01)\n","    (2): ContextModule(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu1): LeakyReLU(negative_slope=0.01)\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu2): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (encoder_block5): Sequential(\n","    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.01)\n","    (2): ContextModule(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu1): LeakyReLU(negative_slope=0.01)\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (relu2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (3): UpsamplingModule(\n","      (layers): Sequential(\n","        (0): Upsample(scale_factor=2.0, mode='nearest')\n","        (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (attention1): AttentionBlock(\n","    (W_gate): Sequential(\n","      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (W_x): Sequential(\n","      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (psi): Sequential(\n","      (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): Sigmoid()\n","    )\n","    (relu): ReLU(inplace=True)\n","  )\n","  (decoder_block1): Sequential(\n","    (0): LocalisationModule(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (conv2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (1): UpsamplingModule(\n","      (layers): Sequential(\n","        (0): Upsample(scale_factor=2.0, mode='nearest')\n","        (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (attention2): AttentionBlock(\n","    (W_gate): Sequential(\n","      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (W_x): Sequential(\n","      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (psi): Sequential(\n","      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): Sigmoid()\n","    )\n","    (relu): ReLU(inplace=True)\n","  )\n","  (localisation2): LocalisationModule(\n","    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (conv2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (up3): UpsamplingModule(\n","    (layers): Sequential(\n","      (0): Upsample(scale_factor=2.0, mode='nearest')\n","      (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n","  (attention3): AttentionBlock(\n","    (W_gate): Sequential(\n","      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (W_x): Sequential(\n","      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (psi): Sequential(\n","      (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): Sigmoid()\n","    )\n","    (relu): ReLU(inplace=True)\n","  )\n","  (localisation3): LocalisationModule(\n","    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (conv2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (up4): UpsamplingModule(\n","    (layers): Sequential(\n","      (0): Upsample(scale_factor=2.0, mode='nearest')\n","      (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n","  (attention4): AttentionBlock(\n","    (W_gate): Sequential(\n","      (0): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (W_x): Sequential(\n","      (0): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (psi): Sequential(\n","      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): Sigmoid()\n","    )\n","    (relu): ReLU(inplace=True)\n","  )\n","  (final_conv): Sequential(\n","    (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.01)\n","    (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (segmentation1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n","  (upscale1): Upsample(scale_factor=2.0, mode='nearest')\n","  (segmentation2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n","  (upscale2): Upsample(scale_factor=2.0, mode='nearest')\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["model=build_model()\n","load_checkpoint(torch.load(CFG.checkpoint_dir), model)\n","# model.load_state_dict(tc.load(CFG.model_path[ model_path_i ],\"cpu\"))\n","model.eval()\n","# model=DataParallel(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T11:42:12.950558Z","iopub.status.busy":"2024-01-12T11:42:12.950197Z","iopub.status.idle":"2024-01-12T11:42:13.136994Z","shell.execute_reply":"2024-01-12T11:42:13.135805Z","shell.execute_reply.started":"2024-01-12T11:42:12.950526Z"},"trusted":true},"outputs":[],"source":["def get_output(debug=False):\n","    outputs=[]\n","    \n","    if debug:\n","        # paths=[\"/kaggle/input/blood-vessel-segmentation/train/kidney_2\"]\n","        paths=[\"../../data/train/kidney_2\"]\n","    else:\n","        paths=glob(\"/kaggle/input/blood-vessel-segmentation/test/*\")\n","    outputs=[[],[]]\n","    for path in [ paths[CFG.path_submition] ]:\n","        x=load_data(path,\"/images/\")\n","        labels=tc.zeros_like(x,dtype=tc.uint8)\n","        mark=Pipeline_Dataset(x,path).get_marks()\n","        \n","        \n","        for axis in [0,1,2]:\n","            debug_count=0\n","            if axis==0:\n","                x_=x\n","                labels_=labels\n","            elif axis==1:\n","                x_=x.permute(1,2,0)\n","                labels_=labels.permute(1,2,0)\n","            elif axis==2:\n","                x_=x.permute(2,0,1)\n","                labels_=labels.permute(2,0,1)\n","            if x.shape[0]==3 and axis!=0:\n","                break\n","            dataset=Pipeline_Dataset(x_,path)\n","            dataloader=DataLoader(dataset,batch_size=1,shuffle=False,num_workers=1)\n","            shape=dataset.x.shape[-2:]\n","            x1_list = np.arange(0, shape[0]+CFG.tile_size-CFG.tile_size+1, CFG.stride)\n","            y1_list = np.arange(0, shape[1]+CFG.tile_size-CFG.tile_size+1, CFG.stride)\n","            for img,index in tqdm(dataloader):\n","                #img=(1,C,H,W)\n","                img=img.to(\"cuda:0\")\n","                img=add_edge(img[0],CFG.tile_size//2)[None]\n","\n","                mask_pred = tc.zeros_like(img[:,0],dtype=tc.float32,device=img.device)\n","                mask_count = tc.zeros_like(img[:,0],dtype=tc.float32,device=img.device)\n","\n","                indexs=[]\n","                chip=[]\n","                for y1 in y1_list:\n","                    for x1 in x1_list:\n","                        x2 = x1 + CFG.tile_size\n","                        y2 = y1 + CFG.tile_size\n","                        indexs.append([x1+CFG.drop_egde_pixel,x2-CFG.drop_egde_pixel,\n","                                       y1+CFG.drop_egde_pixel,y2-CFG.drop_egde_pixel])\n","                        chip.append(img[...,x1:x2,y1:y2].to(torch.float32))\n","                        \n","                t = torch.cuda.get_device_properties(0).total_memory / 1024**3\n","                a = torch.cuda.memory_allocated(0) / 1024**3\n","                print(f'Chip data type = {type(chip[0][0][0][0][0].item())}')\n","                print(f'Made it to batch image {index}/{len(dataloader)}, MEMORY USAGE: {a:.2f}GB out of {t:.2f}GB ({a/t*100:.2f}%)')\n","                y_preds = model.forward(tc.cat(chip)).to(device=0)\n","                ## sigmoid\n","                y_preds = torch.sigmoid(y_preds)\n","\n","                if CFG.drop_egde_pixel:\n","                    y_preds=y_preds[...,CFG.drop_egde_pixel:-CFG.drop_egde_pixel,\n","                                        CFG.drop_egde_pixel:-CFG.drop_egde_pixel]\n","                for i,(x1,x2,y1,y2) in enumerate(indexs):\n","                    mask_pred[...,x1:x2, y1:y2] += y_preds[i]\n","                    mask_count[...,x1:x2, y1:y2] += 1\n","\n","                mask_pred /= mask_count\n","\n","                #Rrecover\n","                mask_pred=mask_pred[...,CFG.tile_size//2:-CFG.tile_size//2,CFG.tile_size//2:-CFG.tile_size//2]\n","                \n","                labels_[index]+=(mask_pred[0]*255 /3 ).to(tc.uint8).cpu()\n","                if debug:\n","                    debug_count+=1\n","                    plt.subplot(121)\n","                    plt.imshow(img[0,CFG.in_chans//2].cpu().detach().numpy())\n","                    plt.subplot(122)\n","                    plt.imshow(mask_pred[0].cpu().detach().numpy())\n","                    plt.show()\n","                    if debug_count>3:\n","                        break\n","        outputs[0].append(labels)\n","        outputs[1].extend(mark)\n","    return outputs"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-12T11:42:13.139308Z","iopub.status.busy":"2024-01-12T11:42:13.138536Z","iopub.status.idle":"2024-01-12T11:44:11.593250Z","shell.execute_reply":"2024-01-12T11:44:11.591726Z","shell.execute_reply.started":"2024-01-12T11:42:13.139273Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/139 [00:00<?, ?it/s]"]}],"source":["# is_submit=len(glob(\"/kaggle/input/blood-vessel-segmentation/test/kidney_5/images/*.tif\"))!=3\n","is_submit=False\n","output,ids=get_output(not is_submit)\n","print('got output')\n","\n","####################################\n","TH=[x.flatten().numpy() for x in output]\n","TH=np.concatenate(TH)\n","index = -int(len(TH) * CFG.th_percentile)\n","TH:int = np.partition(TH, index)[index]\n","print(TH)\n","\n","# img=cv2.imread(\"/kaggle/input/blood-vessel-segmentation/test/kidney_5/images/0001.tif\",cv2.IMREAD_GRAYSCALE)\n","img=cv2.imread(\"../../data/train/kidney_2/images/0001.tif\",cv2.IMREAD_GRAYSCALE)\n","\n","\n","####################################\n","submission_df=[]\n","debug_count=0\n","for index in range(len(ids)):\n","    id=ids[index]\n","    i=0\n","    for x in output:\n","        if index>=len(x):\n","            index-=len(x)\n","            i+=1\n","        else:\n","            break\n","    mask_pred=(output[i][index]>TH).numpy()\n","    \n","    mask_pred2 = to_original ( mask_pred, img, image_size = CFG.image_size )\n","    mask_pred =  mask_pred2.copy()\n","    \n","    ####################################\n","    if not is_submit:\n","        plt.subplot(121)\n","        plt.imshow(mask_pred)\n","        plt.show()\n","        debug_count+=1\n","        if debug_count>6:\n","            break\n","        \n","    rle = rle_encode(mask_pred)\n","    \n","    submission_df.append(\n","        pd.DataFrame(data={\n","            'id'  : id,\n","            'rle' : rle,\n","        },index=[0])\n","    )\n","\n","submission_df =pd.concat(submission_df)\n","submission_df.to_csv('submission.csv', index=False)\n","submission_df.head(6)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6962461,"sourceId":61446,"sourceType":"competition"},{"datasetId":4087873,"sourceId":7187369,"sourceType":"datasetVersion"},{"datasetId":4192036,"sourceId":7238335,"sourceType":"datasetVersion"},{"datasetId":4229452,"sourceId":7312958,"sourceType":"datasetVersion"},{"datasetId":4243245,"sourceId":7317236,"sourceType":"datasetVersion"},{"datasetId":4249424,"sourceId":7322246,"sourceType":"datasetVersion"},{"datasetId":4250312,"sourceId":7340169,"sourceType":"datasetVersion"},{"sourceId":150248402,"sourceType":"kernelVersion"},{"sourceId":156694315,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
