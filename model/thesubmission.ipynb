{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Global Params"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import cv2\n","from glob import glob\n","import math\n","from tqdm import tqdm\n","import shutil\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import albumentations as A\n","from torchvision import transforms\n","import tifffile as tiff\n","import time\n","import torch.nn.functional as F\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import shutil"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T07:24:36.190731Z","iopub.status.busy":"2024-01-18T07:24:36.190171Z","iopub.status.idle":"2024-01-18T07:24:36.197445Z","shell.execute_reply":"2024-01-18T07:24:36.196252Z","shell.execute_reply.started":"2024-01-18T07:24:36.190695Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Global parameters for the UNet model defined here for easy access across files.\n","\"\"\"\n","# Copied from COMP3710 report\n","\n","# Hyperparameters\n","LEARNING_RATE = 1e-4\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 30\n","NUM_WORKERS = 4\n","PIN_MEMORY = True\n","# PREDICTION_THRESHOLD = 0.5\n","PREDICTION_THRESHOLD = 0.99\n","\n","IMAGE_HEIGHT = 512\n","IMAGE_WIDTH = 512\n","\n","HIGH_PASS_ALPHA = 0.1\n","HIGH_PASS_STRENGTH = 0.1\n","\n","TILE_SIZE = 512\n","\n","TILES_IN_X = 4\n","TILES_IN_Y = 5\n","\n","test_dir = '/kaggle/input/blood-vessel-segmentation/test'\n","IS_LOCAL = not os.path.exists(test_dir)\n","IS_SUBMISSION = (not IS_LOCAL) and len(glob(test_dir+\"/kidney_5/images/*.tif\"))!=3\n","\n","CHECKPOINT_DIR = '../../checkpoints/checkpoint.pth.tar'\n","VAL_DATASET_DIR = '../../data/train/kidney_2'\n","\n","IMG_FILE_EXT = '.tif'\n","MASK_FILE_EXT = '.tif'\n","\n","# base_path = 'data/train'\n","base_path = 'data_downsampled512/train'\n","# dataset = 'kidney_1_dense'\n","datasets = ['kidney_1_dense', 'kidney_1_voi', 'kidney_2', 'kidney_3_sparse']"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T07:24:36.352083Z","iopub.status.busy":"2024-01-18T07:24:36.351485Z","iopub.status.idle":"2024-01-18T07:24:36.391682Z","shell.execute_reply":"2024-01-18T07:24:36.390688Z","shell.execute_reply.started":"2024-01-18T07:24:36.352042Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Code copied from https://www.kaggle.com/code/aniketkolte04/sennet-hoa-seg-pytorch-attention-gated-unet\n","pytorch dataset for the challenge.\n","\"\"\"\n","class CustomDataset(Dataset):\n","    def __init__(self, image_files, mask_files, \n","                 input_size=(IMAGE_WIDTH, IMAGE_HEIGHT), \n","                 augmentation_transforms=None):\n","        self.image_files = image_files\n","        self.mask_files = mask_files\n","        self.input_size = input_size\n","        self.augmentation_transforms = augmentation_transforms\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","       \n","        image_path = self.image_files[idx]\n","        mask_path = self.mask_files[idx]\n","\n","        image = preprocess_image(image_path)\n","        mask = preprocess_mask(mask_path)\n","\n","        if self.augmentation_transforms:\n","            image, mask = self.augmentation_transforms(image, mask)\n","\n","        return image, mask\n","\n","class UsageDataset(Dataset):\n","    def __init__(self, image_files, \n","                 input_size=(IMAGE_WIDTH, IMAGE_HEIGHT), \n","                 augmentation_transforms=None):\n","        self.image_files = image_files\n","        self.input_size = input_size\n","        self.augmentation_transforms = augmentation_transforms\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","       \n","        image_path = self.image_files[idx]\n","\n","        image, orig_size = preprocess_image(image_path, return_size=True)\n","        # orig_size = image.shape\n","\n","        if self.augmentation_transforms:\n","            image = self.augmentation_transforms(image)\n","\n","        return image, torch.tensor(np.array([orig_size[0], orig_size[1]]))\n","\n","\n","def preprocess_image(path, return_size=False):\n","    \n","    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","    # img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","    # print(f'fresh process image img.shape: {img.shape}')\n","    img = np.tile(img[...,None],[1, 1, 3]) \n","    img = img.astype('float32') \n","\n","    # scaling to 0-1\n","    mx = np.max(img)\n","    if mx:\n","        img/=mx\n","    \n","    orig_size = img.shape\n","    \n","    # print(f'process image img.shape: {img.shape}')\n","    img = np.transpose(img, (2, 0, 1))\n","    img_ten = torch.tensor(img)\n","    if return_size:\n","        return img_ten, orig_size\n","    else:\n","        return img_ten\n","\n","\n","def preprocess_mask(path):\n","    \n","    msk = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","    msk = msk.astype('float32')\n","    msk/=255.0\n","    msk_ten = torch.tensor(msk)\n","    \n","    return msk_ten\n","\n","def augment_image(image, mask):\n","    \n","    image_np = image.permute(1, 2, 0).numpy()\n","    mask_np = mask.numpy()\n","\n","    transform = A.Compose([\n","        A.RandomCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, always_apply=True),\n","        A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH, interpolation=cv2.INTER_NEAREST),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.ShiftScaleRotate(scale_limit=(-0.1, 0.4), rotate_limit=15, shift_limit=0.1, p=0.8, border_mode=0),\n","        A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n","        A.OneOf(\n","            [\n","                A.Blur(blur_limit=3, p=1),\n","                A.MotionBlur(blur_limit=3, p=1),\n","            ],\n","            p=0.7,\n","        ),\n","        A.Emboss(alpha=HIGH_PASS_ALPHA, strength=HIGH_PASS_STRENGTH, always_apply=True),  # High pass filter\n","    ])\n","\n","    augmented = transform(image=image_np, mask=mask_np)\n","    augmented_image, augmented_mask = augmented['image'], augmented['mask']\n","\n","    augmented_image = torch.tensor(augmented_image, dtype=torch.float32).permute(2, 0, 1)\n","    augmented_mask = torch.tensor(augmented_mask, dtype=torch.float32)\n","\n","    return augmented_image, augmented_mask\n","\n","def val_transform(image, mask):\n","    \n","    image_np = image.permute(1, 2, 0).numpy()\n","    mask_np = mask.numpy()\n","\n","    transform = A.Compose([\n","        A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH, interpolation=cv2.INTER_NEAREST),\n","        A.Emboss(alpha=HIGH_PASS_ALPHA, strength=HIGH_PASS_STRENGTH, always_apply=True),  # High pass filter\n","    ])\n","\n","    augmented = transform(image=image_np, mask=mask_np)\n","    augmented_image, augmented_mask = augmented['image'], augmented['mask']\n","\n","    augmented_image = torch.tensor(augmented_image, dtype=torch.float32).permute(2, 0, 1)\n","    augmented_mask = torch.tensor(augmented_mask, dtype=torch.float32)\n","\n","    return augmented_image, augmented_mask\n","\n","def create_loader(image_files, mask_files, batch_size, \n","                  augmentations=None, shuffle=False):\n","    \n","    dataset = CustomDataset(image_files, mask_files, augmentation_transforms=augmentations)\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","\n","def create_test_loader(image_files, batch_size, \n","                  augmentations=None, shuffle=False):\n","    \n","    dataset = UsageDataset(image_files, augmentation_transforms=augmentations)\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T07:24:36.394335Z","iopub.status.busy":"2024-01-18T07:24:36.393921Z","iopub.status.idle":"2024-01-18T07:24:36.438624Z","shell.execute_reply":"2024-01-18T07:24:36.437842Z","shell.execute_reply.started":"2024-01-18T07:24:36.394300Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","A CNN model based on the Improved UNet architecture, with associated modules.\n","\"\"\"\n","# This is my 2D UNet from COMP3710\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","\n","class ContextModule(nn.Module):\n","    \"\"\"\n","    This is the context module from the improved UNet architecture.\n","    \"Each context module is in fact a pre-activation residual block with two\n","    3x3 convolutional layers and a dropout layer (pdrop = 0.3) in between.\"\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Initialize the ContextModule.\n","        \"\"\"\n","        super(ContextModule, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.norm1 = nn.InstanceNorm2d(out_channels)\n","        self.relu1 = nn.LeakyReLU(negative_slope=1e-2)\n","        self.dropout = nn.Dropout2d(p=0.3)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.norm2 = nn.InstanceNorm2d(out_channels)\n","        self.relu2 = nn.LeakyReLU(negative_slope=1e-2)\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the ContextModule.\n","        \"\"\"\n","        out = self.conv1(x)\n","        out = self.norm1(out)\n","        out = self.relu1(out)\n","        out = self.dropout(out)\n","        out = self.conv2(out)\n","        out = self.norm2(out)\n","        out = self.relu2(out)\n","        out += x # residual connection\n","        return out\n","\n","class LocalisationModule(nn.Module):\n","    \"\"\"\n","    This is the localization module from the improved UNet architecture.\n","    A localization module consists of a 3x3 convolution followed by a 1x1 convolution that \n","    halves the number of feature maps.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Initialize the LocalisationModule.\n","        \"\"\"\n","        super(LocalisationModule, self).__init__()\n","        # self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n","        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","    \n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the LocalisationModule.\n","        \"\"\"\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        return out\n","\n","\n","class UpsamplingModule(nn.Module):\n","    \"\"\"\n","    An upsampling module consists of an upsampling layer that repeats the feature pixels \n","    twice in each spatial dimension followed by a 3x3 convolution.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Initialize the UpsamplingModule.\n","        \"\"\"\n","        super(UpsamplingModule, self).__init__()\n","        self.layers = nn.Sequential(\n","            nn.Upsample(scale_factor=2, mode='nearest'),\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        )\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the UpsamplingModule.\n","        \"\"\"\n","        out = self.layers(x)\n","        return out\n","\n","class AttentionBlock(nn.Module):\n","    \"\"\"Attention block with learnable parameters\"\"\"\n","\n","    def __init__(self, F_g, F_l, n_coefficients):\n","        \"\"\"\n","        :param F_g: number of feature maps (channels) in previous layer\n","        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n","        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n","        \"\"\"\n","        super(AttentionBlock, self).__init__()\n","\n","        self.W_gate = nn.Sequential(\n","            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(n_coefficients)\n","        )\n","\n","        self.W_x = nn.Sequential(\n","            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(n_coefficients)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, gate, skip_connection):\n","        \"\"\"\n","        :param gate: gating signal from previous layer\n","        :param skip_connection: activation from corresponding encoder layer\n","        :return: output activations\n","        \"\"\"\n","        g1 = self.W_gate(gate)\n","        x1 = self.W_x(skip_connection)\n","        psi = self.relu(g1 + x1)\n","        psi = self.psi(psi)\n","        out = skip_connection * psi\n","        return out\n","\n","\n","class ImprovedUNet(nn.Module):\n","    \"\"\"\n","    This is the improved UNet model, it consists of the context aggregation pathway (encoder)\n","    and the localization pathway (decoder). The model is designed to outpreform the original\n","    UNet for medical image segmentation tasks.\n","    See https://arxiv.org/pdf/1802.10508v1.pdf for network architecture.\n","    \"\"\"\n","    def __init__(self, in_channels=3, out_channels=1, features=[16,32,64,128,256]):\n","        \"\"\"\n","        Initialize the ImprovedUNet model by creating all necessary layers.\n","\n","        Args:\n","            in_channels (int, optional): Number of input channels. Defaults to 3 for RGB color images.\n","            out_channels (int, optional): Number of output channels. Defaults to 1 a for greyscale binary \n","            segmentation mask.\n","            features (list, optional): The numbers of feature maps to extract (must be length 5 and each \n","            entry must be double the previous entry). Defaults to [16,32,64,128,256].\n","        \"\"\"\n","        super(ImprovedUNet, self).__init__()\n","        self.encoder_block1 = nn.Sequential(\n","            nn.Conv2d(in_channels, features[0], kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[0], features[0]), # 3 channels in, 16 channels out\n","        )\n","        self.encoder_block2 = nn.Sequential(\n","            nn.Conv2d(features[0], features[1], kernel_size=3, stride=2, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[1], features[1]), # 16 channels in, 32 channels out\n","        )\n","        self.encoder_block3 = nn.Sequential(\n","            nn.Conv2d(features[1], features[2], kernel_size=3, stride=2, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[2], features[2]), # 32 channels in, 64 channels out\n","        )\n","        self.encoder_block4 = nn.Sequential(\n","            nn.Conv2d(features[2], features[3], kernel_size=3, stride=2, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[3], features[3]), # 64 channels in, 128 channels out\n","        )\n","        self.encoder_block5 = nn.Sequential(\n","            nn.Conv2d(features[3], features[4], kernel_size=3, stride=2, padding=1),\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            ContextModule(features[4], features[4]), # 128 channels in, 256 channels out\n","            # upsampling module in last encoder block increases spatial dimensions by 2 for decoder\n","            UpsamplingModule(features[4], features[3]), # 256 channels in, 128 channels out\n","        )\n","        \n","        # Adding in attention blocks gates at the concatenation of the skip connection and the bottleneck layer\n","        self.attention1 = AttentionBlock(features[3], features[3], 32)\n","        \n","        # Upsampling modules half the number of feature maps but after upsampling, the output\n","        # is concatenated with the skip connection, so the number of feature maps is doubled\n","        # the localization modules then halve the number of feature maps again\n","        self.decoder_block1 = nn.Sequential(\n","            LocalisationModule(features[4], features[3]), # 256 channels in, 128 channels out\n","            UpsamplingModule(features[3], features[2]), # 128 channels in, 64 channels out\n","        )\n","        self.attention2 = AttentionBlock(features[2], features[2], 64)\n","        # these decoder layers need to be split up to allow for skip connections\n","        self.localisation2 = LocalisationModule(features[3], features[2]) # 128 channels in, 64 channels out\n","        self.up3 = UpsamplingModule(features[2], features[1]) # 64 channels in, 32 channels out, double spatial dimensions\n","        self.attention3 = AttentionBlock(features[1], features[1], 128)\n","        self.localisation3 = LocalisationModule(features[2], features[1]) # 64 channels in, 32 channels out\n","        self.up4 = UpsamplingModule(features[1], features[0]) # 32 channels in, 16 channels out, double spatial dimensions\n","        self.attention4 = AttentionBlock(features[0], features[0], 256)\n","        self.final_conv = nn.Sequential(\n","            nn.Conv2d(features[1], features[1], kernel_size=1), # 32 channels in, 32 channels out, final convolutional layer\n","            nn.LeakyReLU(negative_slope=1e-2),\n","            nn.Conv2d(features[1], out_channels, kernel_size=1) # 32 channels in, 1 channel out, segmentation layer\n","        )\n","        \n","        self.segmentation1 = nn.Conv2d(features[2], out_channels, kernel_size=1) # 64 channels in, 1 channels out, segmentation layer\n","        self.upscale1 = nn.Upsample(scale_factor=2, mode='nearest') # upscale the segmentation layer to match the dimensions of the output\n","        self.segmentation2 = nn.Conv2d(features[1], out_channels, kernel_size=1) # 32 channels in, 1 channels out, segmentation layer\n","        self.upscale2 = nn.Upsample(scale_factor=2, mode='nearest') # upscale the segmentation layer to match the dimensions of the output\n","        \n","    \n","    def forward(self, x):\n","        \"\"\"Forward pass of the ImprovedUNet model to generate a binary segmentation mask.\n","\n","        Args:\n","            x (torch.Tensor): An input image in tensor form.\n","\n","        Returns:\n","            torch.Tensor: Binary segmentation mask in tensor form\n","        \"\"\"\n","        skip_connections = []\n","        \n","        x = self.encoder_block1(x) # 3 channels in, 16 channels out\n","        skip_connections.append(x)\n","        x = self.encoder_block2(x) # 16 channels in, 32 channels out\n","        skip_connections.append(x)\n","        x = self.encoder_block3(x) # 32 channels in, 64 channels out\n","        skip_connections.append(x)\n","        x = self.encoder_block4(x) # 64 channels in, 128 channels out\n","        skip_connections.append(x)\n","        # bottleneck layer\n","        x = self.encoder_block5(x) # 128 channels in, 128 channels out\n","        \n","        # use skip connections as a stack to allow for easy popping\n","        # concatenate the skip connection with 128 channels with the bottleneck layer with 128 channels\n","        # x = torch.cat((x, skip_connections.pop()), dim=1)\n","        \n","        # First attention block\n","        a = self.attention1(x, skip_connections.pop())\n","        x = torch.cat((a, x), dim=1)\n","        \n","        x = self.decoder_block1(x) # 256 channels in, 64 channels out\n","        # concatenate the skip connection with 64 channels with the bottleneck layer with 64 channels\n","        # x = torch.cat((x, skip_connections.pop()), dim=1)\n","        \n","        # Second attention block\n","        a = self.attention2(x, skip_connections.pop())\n","        x = torch.cat((a, x), dim=1)\n","        \n","        x = self.localisation2(x) # 128 channels in, 64 channels out\n","        # additional skip connections for segmentation layers in the decoder\n","        seg_connection1 = self.segmentation1(x) # 64 channels in, 1 channel out\n","        seg_connection1 = self.upscale1(seg_connection1)\n","        \n","        x = self.up3(x)\n","        \n","        # concatenate the skip connection with 32 channels with the bottleneck layer with 32 channels\n","        # x = torch.cat((x, skip_connections.pop()), dim=1)\n","        \n","        # Third attention block\n","        a = self.attention3(x, skip_connections.pop())\n","        x = torch.cat((a, x), dim=1)\n","        \n","        x = self.localisation3(x) # 64 channels in, 32 channels out\n","        # additional skip connections for segmentation layers in the decoder\n","        seg_connection2 = self.segmentation2(x) # 32 channels in, 1 channel out\n","        # element wise addition of the segmentation connections\n","        seg_connection2 += seg_connection1\n","        seg_connection2 = self.upscale2(seg_connection2) # upscale the segmentation connection to match the dimensions of the output\n","        \n","        x = self.up4(x)\n","        # concatenate the skip connection with 16 channels with the bottleneck layer with 16 channels\n","        # x = torch.cat((x, skip_connections.pop()), dim=1)\n","        \n","        # Fourth attention block\n","        a = self.attention4(x, skip_connections.pop())\n","        x = torch.cat((a, x), dim=1)\n","        \n","        x = self.final_conv(x) # 32 channels in, 1 channel out, includes final convolutional layer and segmentation layer\n","        \n","        # element wise addition of the segmentation connections\n","        x += seg_connection2\n","        \n","        # use sigmoid activation function to squash the output to between 0 and 1\n","        # x = torch.sigmoid(x)\n","        return x\n","        # return x # no final activation function\n"]},{"cell_type":"markdown","metadata":{},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T07:24:36.440447Z","iopub.status.busy":"2024-01-18T07:24:36.440150Z","iopub.status.idle":"2024-01-18T07:24:36.481849Z","shell.execute_reply":"2024-01-18T07:24:36.480987Z","shell.execute_reply.started":"2024-01-18T07:24:36.440422Z"},"trusted":true},"outputs":[],"source":["import torchvision\n","\n","def save_checkpoint(state, filename='checkpoints/checkpoint.pth.tar'):\n","    \"\"\"\n","    Saves the model and optimizer state dicts to a checkpoint file\n","    \"\"\"\n","    print('>>> Saving checkpoint')\n","    # os.makedirs('checkpoints', exist_ok=True)\n","    torch.save(state, filename)\n","    print('>>> Checkpoint saved')\n","\n","def load_checkpoint(checkpoint, model, optimizer=None):\n","    \"\"\"\n","    Loads the model and optimizer state dicts from a checkpoint file\n","    \"\"\"\n","    print('>>> Loading checkpoint')\n","    model.load_state_dict(checkpoint['state_dict'])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","    print('>>> Checkpoint loaded')\n","\n","def remove_small_objects(img, min_size):\n","    # Find all connected components (labels)\n","    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n","\n","    # Create a mask where small objects are removed\n","    new_img = np.zeros_like(img)\n","    for label in range(1, num_labels):\n","        if stats[label, cv2.CC_STAT_AREA] >= min_size:\n","            new_img[labels == label] = 1\n","\n","    return new_img\n","\n","def rle_encode(mask, bg = 0) -> dict:\n","    vec = mask.flatten()\n","    nb = len(vec)\n","    where = np.flatnonzero\n","    starts = np.r_[0, where(~np.isclose(vec[1:], vec[:-1], equal_nan=True)) + 1]\n","    lengths = np.diff(np.r_[starts, nb])\n","    values = vec[starts]\n","    assert len(starts) == len(lengths) == len(values)\n","    rle = []\n","    for start, length, val in zip(starts, lengths, values):\n","        if val == bg:\n","            continue\n","        rle += [str(start), length]\n","    # post-processing\n","    return \" \".join(map(str, rle))\n","\n","def create_rle_df(kidney_dirs: [str], \n","                  subdir_name: str = 'preds',\n","                  preds_path=None,\n","                  img_size=(512, 512),\n","                  resize=True,\n","                  orig_path='/kaggle/input/blood-vessel-segmentation/test/'):\n","    \"\"\"\n","    Creates a dataframe with the image ids and the predicted masks\n","    \"\"\"\n","    if type(kidney_dirs) == str:\n","        kidney_dirs = [kidney_dirs]\n","    \n","    df = pd.DataFrame(columns=['id', 'rle'])\n","    # df.set_index('id', inplace=True)\n","        \n","    for kidney_dir in kidney_dirs:\n","        assert os.path.exists(kidney_dir), f'{kidney_dir} does not exist'\n","        \n","        kidney_name = os.path.basename(kidney_dir)\n","        print(f'KIDNEY NAME: {kidney_name}')\n","        \n","        if resize:\n","            example_path = os.path.join(orig_path, kidney_name, 'images', '0000', IMG_FILE_EXT)\n","            assert os.path.exists(example_path), f'{example_path} does not exist'\n","            orig_size = cv2.imread(example_path).shape[:2]\n","            print(f'KIDNEY IMG SIZE: {orig_size}')\n","            img_size = (orig_size[1], orig_size[0])\n","        \n","        if preds_path == None:\n","            masks = sorted([os.path.join(kidney_dir, subdir_name, f) for f in \n","                            os.listdir(os.path.join(kidney_dir, subdir_name)) if f.endswith(MASK_FILE_EXT)])\n","        else:\n","            masks = sorted([os.path.join(preds_path, f) for f in \n","                            os.listdir(os.path.join(preds_path)) if f.endswith(MASK_FILE_EXT)])\n","        \n","        for mask_file in masks:\n","            mask_name = os.path.basename(mask_file)\n","            mask = Image.open(mask_file).convert('L').resize(img_size, resample=Image.BOX)\n","            \n","#             plt.imshow(mask)\n","            \n","            mask = np.array(mask, dtype=np.float32)\n","#             mask[mask==1] = 255\n","#             mask = remove_small_objects(mask, 5) # added this to try and fix scoring error \n","\n","#             print(f'MASK SHAPE: {mask.shape}, MASK MAX: {mask.max()}')\n","            \n","            \n","            id = f'{kidney_name}_{mask_name[:-4]}'\n","            rle = rle_encode(mask)\n","#             print(f'ID --------------> {id}')\n","#             print(f'RLE -------------> {rle}')\n","#             print(f'RLE TYPE: {type(rle)}')\n","#             print('------------ END OF ENTRY ------------')\n","            if rle == '':\n","                rle = '1 0'\n","            # df.loc[id] = rle\n","            df.loc[len(df)] = [id, rle]\n","        \n","    # =========== MIGHT WANT TO REMOVE THIS ===============\n","#     df['width'] = [img_size[0]] * len(df)\n","#     df['height'] = [img_size[1]] * len(df)\n","    \n","    return df\n","\n","def save_predictions(kidney_dirs, model, num='all', min_idx=0,\n","                     folder='saved_images/', device='cuda', \n","                     verbose=True, image_size=(IMAGE_HEIGHT, IMAGE_WIDTH)):\n","    \"\"\"\n","    Saves the predictions from the model as images in the folder\n","    \"\"\"\n","    if type(kidney_dirs) == str:\n","        kidney_dirs = [kidney_dirs]\n","\n","    for kidney_dir in kidney_dirs:\n","        assert os.path.exists(kidney_dir), f'{kidney_dir} does not exist'\n","        \n","        kidney_name = os.path.basename(kidney_dir)\n","        \n","        preds_path = os.path.join(folder, kidney_name, 'preds')\n","#         masks_path = os.path.join(folder, kidney_name, 'labels')\n","        images_path = os.path.join(folder, kidney_name, 'images')\n","        os.makedirs(preds_path, exist_ok=True)\n","#         os.makedirs(masks_path, exist_ok=True)\n","        os.makedirs(images_path, exist_ok=True)\n","        \n","#         masks = sorted([os.path.join(kidney_dir, 'labels', f) for f in \n","#                         os.listdir(os.path.join(kidney_dir, 'labels')) if f.endswith('.tif')])\n","        images = sorted([os.path.join(kidney_dir, 'images', f) for f in \n","                        os.listdir(os.path.join(kidney_dir, 'images')) if f.endswith(IMG_FILE_EXT)])\n","                \n","        if num == 'all':\n","            num = len(images)\n","        else:\n","            num = min(num, len(images))\n","        \n","        assert min_idx + num <= len(images), f'Index out of range. There are {len(images)} images.'\n","        \n","        # transforms = A.Compose([\n","        #     A.Resize(image_size[0], image_size[1], interpolation=cv2.INTER_NEAREST),\n","        #     A.Emboss(alpha=HIGH_PASS_ALPHA, strength=HIGH_PASS_STRENGTH, always_apply=True),  # High pass filter\n","        #     ToTensorV2()\n","        # ])\n","        loader = create_loader(images, images, batch_size=1, augmentations=val_transform)\n","        model.eval()\n","        print('>>> Generating and saving predictions') if verbose else None\n","        loop = tqdm(enumerate(loader), total=num, leave=False) if verbose else enumerate(loader)\n","        with torch.no_grad():\n","            for idx, (x, y) in loop:\n","                if idx < min_idx:\n","                    continue\n","                \n","                filename = os.path.basename(images[idx])\n","                x = x.to(device)\n","                y = y.to(device) # add 1 channel to mask\n","                preds = torch.sigmoid(model(x))\n","                preds = (preds > PREDICTION_THRESHOLD).float()\n","                torchvision.utils.save_image(preds, os.path.join(preds_path, filename))\n","#                 torchvision.utils.save_image(y.unsqueeze(1), os.path.join(masks_path, filename))\n","                torchvision.utils.save_image(x, os.path.join(images_path, filename))\n","                if idx > min_idx + num:\n","                    break\n","        model.train()\n","\n","        \n","def to_1024(image):\n","    \"\"\"\n","    Converts image to 1024x1024\n","    \"\"\"\n","    image = image.permute(1, 2, 0).numpy()\n","    image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n","    image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)\n","    return image\n","\n","def split(img):\n","    \"\"\"\n","    Expects a 1024x1024 image and splits it into 4 512x512 images\n","    \"\"\"\n","    img = img.permute(1, 2, 0).numpy()\n","    img1 = img[:512, :512, :]\n","    img2 = img[:512, 512:, :]\n","    img3 = img[512:, :512, :]\n","    img4 = img[512:, 512:, :]\n","    img1 = torch.tensor(img1, dtype=torch.float32).permute(2, 0, 1)\n","    img2 = torch.tensor(img2, dtype=torch.float32).permute(2, 0, 1)\n","    img3 = torch.tensor(img3, dtype=torch.float32).permute(2, 0, 1)\n","    img4 = torch.tensor(img4, dtype=torch.float32).permute(2, 0, 1)\n","    return img1, img2, img3, img4\n","\n","def recombine(img1, img2, img3, img4):\n","    \"\"\"\n","    Expects 4 512x512 images and recombines them into a 1024x1024 image\n","    \"\"\"\n","    img1 = img1.permute(1, 2, 0).numpy()\n","    img2 = img2.permute(1, 2, 0).numpy()\n","    img3 = img3.permute(1, 2, 0).numpy()\n","    img4 = img4.permute(1, 2, 0).numpy()\n","    img = np.zeros((1024, 1024, 3))\n","    img[:512, :512, :] = img1\n","    img[:512, 512:, :] = img2\n","    img[512:, :512, :] = img3\n","    img[512:, 512:, :] = img4\n","    img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)\n","    return img\n","\n","def recombine_with_batch_dim(img1, img2, img3, img4):\n","    \"\"\"\n","    Expects 4 512x512 images with batch dimension, [b, c, h, w] and recombines them into a 1024x1024 image\n","    \"\"\"\n","    img1 = img1.permute(0, 2, 3, 1).numpy()\n","    img2 = img2.permute(0, 2, 3, 1).numpy()\n","    img3 = img3.permute(0, 2, 3, 1).numpy()\n","    img4 = img4.permute(0, 2, 3, 1).numpy()\n","    img = np.zeros((img1.shape[0], 1024, 1024, 3))\n","    img[:, :512, :512, :] = img1\n","    img[:, :512, 512:, :] = img2\n","    img[:, 512:, :512, :] = img3\n","    img[:, 512:, 512:, :] = img4\n","    img = torch.tensor(img, dtype=torch.float32).permute(0, 3, 1, 2)\n","    return img\n","\n","def to_1303x912(image):\n","    image = image.permute(1, 2, 0).numpy()\n","    image = cv2.resize(image, (912, 1303), interpolation=cv2.INTER_NEAREST)\n","    image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)\n","    return image\n","\n","\n","def batch_tiling_split(image_batch, tile_size, tiles_in_x=3, tiles_in_y=3, debug=False):\n","    \"\"\"\n","    Splits a batch of images into evenly spaced tiles of size tile_size x tile_size.\n","    Assumes batch shape is [b, c, h, w]\n","    Args:\n","        image_batch: batch of images to split\n","        tile_size: size of tiles to split into\n","        tiles_in_x: number of tiles in x direction\n","        tiles_in_y: number of tiles in y direction\n","    Returns:\n","        A list of tiles\n","    \"\"\"\n","    b, c, h, w = image_batch.shape\n","\n","    # Calculate the overlap to evenly distribute tiles\n","    overlap_x = (tile_size * tiles_in_x - w) // (tiles_in_x - 1) if tiles_in_x > 1 else 0\n","    overlap_y = (tile_size * tiles_in_y - h) // (tiles_in_y - 1) if tiles_in_y > 1 else 0\n","\n","    if debug:\n","        print(f'Image Size: {h}x{w}, Tile Size: {tile_size}x{tile_size}')\n","        print(f'Tiles in x: {tiles_in_x}, Tiles in y: {tiles_in_y}')\n","        print(f'Overlap in x: {overlap_x}, Overlap in y: {overlap_y}')\n","\n","    tiles = []\n","    for i in range(tiles_in_y):\n","        for j in range(tiles_in_x):\n","            # Calculate start coordinates for the tile, considering overlap\n","            start_x = j * (tile_size - overlap_x)\n","            start_y = i * (tile_size - overlap_y)\n","            # Adjust for edge tiles\n","            start_x = min(start_x, w - tile_size)\n","            start_y = min(start_y, h - tile_size)\n","\n","            tile = image_batch[:, :, start_y:start_y + tile_size, start_x:start_x + tile_size]\n","            tiles.append(tile)\n","\n","    return tiles\n","\n","\n","def recombine_tiles(tiles, original_size, tile_size, tiles_in_x, tiles_in_y):\n","    \"\"\"\n","    Recombines a list of tiled predictions into a single prediction batch.\n","    Adjusts edge tiles to fit the original image dimensions.\n","    Args:\n","        tiles: list of tiled predictions\n","        original_size: tuple, size of the original image (height, width)\n","        tile_size: size of each tile\n","        tiles_in_x: number of tiles in the x direction\n","        tiles_in_y: number of tiles in the y direction\n","    Returns:\n","        A single prediction batch of recombined tiles.\n","    \"\"\"\n","    original_height, original_width = original_size\n","    b, c, _, _ = tiles[0].shape\n","\n","    # Create tensors for the recombined prediction and the count for averaging\n","    recombined = torch.zeros((b, c, original_height, original_width), dtype=tiles[0].dtype, device=tiles[0].device)\n","    count_matrix = torch.zeros_like(recombined)\n","\n","    tile_index = 0\n","    for i in range(tiles_in_y):\n","        for j in range(tiles_in_x):\n","            # Calculate start coordinates for the tile\n","            start_x = j * (tile_size - (tile_size * tiles_in_x - original_width) // (tiles_in_x - 1))\n","            start_y = i * (tile_size - (tile_size * tiles_in_y - original_height) // (tiles_in_y - 1))\n","            end_x = min(start_x + tile_size, original_width)\n","            end_y = min(start_y + tile_size, original_height)\n","\n","            # Adjust the size of the tile if it's an edge tile\n","            tile = tiles[tile_index]\n","            if tile.shape[2] != end_y - start_y or tile.shape[3] != end_x - start_x:\n","                tile = tile[:, :, :end_y - start_y, :end_x - start_x]\n","\n","            # Place tile and update count matrix\n","            recombined[:, :, start_y:end_y, start_x:end_x] += tile\n","            count_matrix[:, :, start_y:end_y, start_x:end_x] += 1\n","\n","            tile_index += 1\n","\n","    # Avoid division by zero\n","    count_matrix[count_matrix == 0] = 1\n","\n","    # Average the overlapped regions\n","    recombined /= count_matrix\n","\n","    return recombined"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T07:24:36.512292Z","iopub.status.busy":"2024-01-18T07:24:36.512010Z","iopub.status.idle":"2024-01-18T07:24:36.527810Z","shell.execute_reply":"2024-01-18T07:24:36.526983Z","shell.execute_reply.started":"2024-01-18T07:24:36.512268Z"},"trusted":true},"outputs":[],"source":["def test_transform(image):\n","    \n","    image_np = image.permute(1, 2, 0).numpy()\n","\n","    transform = A.Compose([\n","        # A.Resize(1024, 1024, interpolation=cv2.INTER_NEAREST),\n","        A.Emboss(alpha=HIGH_PASS_ALPHA, strength=HIGH_PASS_STRENGTH, always_apply=True),  # High pass filter\n","    ])\n","\n","    augmented = transform(image=image_np)\n","    augmented_image = augmented['image']\n","\n","    augmented_image = torch.tensor(augmented_image, dtype=torch.float32).permute(2, 0, 1)\n","\n","    return augmented_image\n","\n","def inference(model, device, dataset_folder=\"/kaggle/input/blood-vessel-segmentation\", debug=False):\n","    \"\"\"\n","    adapted from: https://www.kaggle.com/code/kashiwaba/sennet-hoa-inference-unet-simple-baseline#Inference\n","    \"\"\"\n","    if not debug:\n","        ls_images = glob(os.path.join(dataset_folder, \"test\", \"*\", \"*\", \"*\", IMG_FILE_EXT))\n","    else:\n","        ls_images = glob(os.path.join(dataset_folder, \"images\", \"*\", IMG_FILE_EXT))\n","        ls_masks = glob(os.path.join(dataset_folder, \"labels\", \"*\", MASK_FILE_EXT))\n","        ls_images = ls_images[950:1050]\n","        ls_masks = ls_masks[950:1050]\n","    \n","    test_loader = create_test_loader(ls_images, 1, augmentations=test_transform)\n","    \n","    # TTA transforms\n","    hflip_batch = lambda x: torch.flip(x, dims=[3])\n","    vflip_batch = lambda x: torch.flip(x, dims=[2])\n","    vhflip_batch = lambda x: torch.flip(torch.flip(x, dims=[2]), dims=[3])\n","\n","    rles = []\n","    pbar = tqdm(enumerate(test_loader), total=len(test_loader), desc='Inference ')\n","    for step, (images, shapes) in pbar:\n","        shapes = np.array(shapes)\n","        images = images.to(device, dtype=torch.float)\n","        debug = debug and step == 0\n","        ######################\n","        orig_shape = images.shape[2:]\n","        \n","        hf_imgs = hflip_batch(images)\n","        vf_imgs = vflip_batch(images)\n","        vhf_imgs = vhflip_batch(images)\n","        \n","        tiles = batch_tiling_split(images, 512, tiles_in_x=TILES_IN_X, tiles_in_y=TILES_IN_Y)\n","        hf_tiles = batch_tiling_split(hf_imgs, 512, tiles_in_x=TILES_IN_X, tiles_in_y=TILES_IN_Y)\n","        vf_tiles = batch_tiling_split(vf_imgs, 512, tiles_in_x=TILES_IN_X, tiles_in_y=TILES_IN_Y)\n","        vhf_tiles = batch_tiling_split(vhf_imgs, 512, tiles_in_x=TILES_IN_X, tiles_in_y=TILES_IN_Y)\n","        \n","        if debug and step == 0:\n","            print(f'Original shape: {orig_shape}')\n","            print(f'Number of tiles: {len(tiles)}')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            \n","            tile_preds = [model(tile) for tile in tiles]\n","            hf_tile_preds = [model(tile) for tile in hf_tiles]\n","            vf_tile_preds = [model(tile) for tile in vf_tiles]\n","            hvf_tile_preds = [model(tile) for tile in vhf_tiles]\n","            \n","            preds = recombine_tiles(tile_preds, orig_shape, TILE_SIZE, tiles_in_x=TILES_IN_X, tiles_in_y=TILES_IN_Y)\n","            hf_preds = recombine_tiles(hf_tile_preds, orig_shape, TILE_SIZE, tiles_in_x=TILES_IN_X, tiles_in_y=TILES_IN_Y)\n","            vf_preds = recombine_tiles(vf_tile_preds, orig_shape, TILE_SIZE, tiles_in_x=TILES_IN_X, tiles_in_y=TILES_IN_Y)\n","            hvf_preds = recombine_tiles(hvf_tile_preds, orig_shape, TILE_SIZE, tiles_in_x=TILES_IN_X, tiles_in_y=TILES_IN_Y)\n","            \n","            # unflip the TTA predictions\n","            preds += hflip_batch(hf_preds)\n","            preds += vflip_batch(vf_preds)\n","            preds += vhflip_batch(hvf_preds)\n","            preds /= 4\n","            \n","            preds = (nn.Sigmoid()(preds)>PREDICTION_THRESHOLD).double()\n","        preds = preds.cpu().numpy().astype(np.uint8)\n","\n","        for pred in preds:            \n","            # remove channel dimension\n","            pred = pred.squeeze()\n","            \n","            rle = rle_encode(pred)\n","            if rle == '':\n","                rle = '1 0'\n","            rles.append(rle)\n","            \n","            if debug:\n","                # plot an example\n","                mask = preprocess_mask(ls_masks[step]).numpy()\n","                print(f'Prediction shape: {pred.shape}')\n","                plt.figure(figsize=(20, 20))\n","                plt.subplot(1, 3, 1)\n","                plt.imshow(images[0][0].cpu(), cmap='gray')\n","                plt.title('Original Image')\n","                plt.subplot(1, 3, 2)\n","                plt.imshow(mask)\n","                plt.title('True Mask')\n","                plt.subplot(1, 3, 3)\n","                plt.imshow(pred)\n","                plt.title('Prediction')\n","                plt.show()\n","\n","    ids = []\n","    for p_img in tqdm(ls_images):\n","        path_ = p_img.split(os.path.sep)\n","        # parse the submission ID\n","        dataset = path_[-3]\n","        slice_id, _ = os.path.splitext(path_[-1])\n","        ids.append(f\"{dataset}_{slice_id}\")\n","    \n","    submission = pd.DataFrame.from_dict({\n","        \"id\": ids,\n","        \"rle\": rles\n","    })\n","    \n","    return submission"]},{"cell_type":"markdown","metadata":{},"source":["# Prediction"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T07:24:36.530550Z","iopub.status.busy":"2024-01-18T07:24:36.530205Z","iopub.status.idle":"2024-01-18T07:24:38.386488Z","shell.execute_reply":"2024-01-18T07:24:38.385620Z","shell.execute_reply.started":"2024-01-18T07:24:36.530509Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> Loading checkpoint\n",">>> Checkpoint loaded\n"]},{"name":"stderr","output_type":"stream","text":["Inference : 0it [00:00, ?it/s]\n","0it [00:00, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["INFERENCE SUCCESSFULL\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["\"\"\"\n","Coppied from https://www.kaggle.com/code/kashiwaba/sennet-hoa-inference-unet-simple-baseline\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","tesd_dir = '/kaggle/input/blood-vessel-segmentation/test/'\n","\n","def main():\n","    model = ImprovedUNet(in_channels=3, out_channels=1).to(device=device)\n","    load_checkpoint(torch.load(CHECKPOINT_DIR), model)\n","    \n","    # kidney_paths = glob(os.path.join(tesd_dir, 'kidney_*'))\n","#     kidney_paths = ['/kaggle/input/blood-vessel-segmentation/train/kidney_1_voi']\n","    \n","#     try:\n","#         save_predictions(kidney_paths, model, folder='', device=device)\n","#     except:\n","#         print('failure in saving and generating predictions')\n","\n","    # kidney_names = [k.split('/')[-1] for k in kidney_paths]\n","\n","    prediction_rles = inference(model, device, dataset_folder=VAL_DATASET_DIR, debug=True)\n","\n","#     # Deleting the temporary files\n","#     for k in kidney_names:\n","#         shutil.rmtree(k, ignore_errors=True)\n","\n","    prediction_rles.to_csv('submission.csv', index=False)\n","    \n","    print('INFERENCE SUCCESSFULL')\n","\n","\n","if __name__ == '__main__':\n","    main()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":6962461,"sourceId":61446,"sourceType":"competition"},{"datasetId":4192036,"sourceId":7238335,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
