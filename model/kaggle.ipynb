{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sampo\\OneDrive\\Documents\\Documents2\\rendom_prijects\\VasculatureSegmentation\\3DImprovedUNet\\model\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Global parameters for the UNet model defined here for easy access across files.\n",
    "\"\"\"\n",
    "# Copied from COMP3710 report\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "PREDICTION_THRESHOLD = 0.8\n",
    "# PREDICTION_THRESHOLD = 0.999\n",
    "\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 512\n",
    "\n",
    "HIGH_PASS_ALPHA = 0.1\n",
    "HIGH_PASS_STRENGTH = 0.1\n",
    "\n",
    "IS_LOCAL = True#False\n",
    "IS_SUBMISSION = False#True\n",
    "DATA_SUBSECTION = (960, 1080)\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "if IS_LOCAL:\n",
    "    CHECKPOINT_DIR = '../../checkpoints/checkpoint.pth.tar'\n",
    "else:\n",
    "    CHECKPOINT_DIR = '/kaggle/input/checkpoint/checkpoints/checkpoint.pth.tar'\n",
    "\n",
    "if not IS_SUBMISSION:\n",
    "    VAL_DATASET_DIR = '../../data/train/kidney_2'\n",
    "\n",
    "# base_path = 'data/train'\n",
    "# base_path = 'data_downsampled512/train'\n",
    "# dataset = 'kidney_1_dense'\n",
    "# datasets = ['kidney_1_dense', 'kidney_1_voi', 'kidney_2', 'kidney_3_sparse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code copied from https://www.kaggle.com/code/aniketkolte04/sennet-hoa-seg-pytorch-attention-gated-unet\n",
    "pytorch dataset for the challenge.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import tifffile as tiff\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "\n",
    "# project_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "# print(f'project_dir: {project_dir}')\n",
    "# os.chdir(project_dir) # change to project directory\n",
    "\n",
    "# images_path = os.path.join(base_path, dataset, 'images')\n",
    "# labels_path = os.path.join(base_path, dataset, 'labels')\n",
    "\n",
    "# image_files = sorted([os.path.join(images_path, f) for f in os.listdir(images_path) if f.endswith('.tif')])\n",
    "# label_files = sorted([os.path.join(labels_path, f) for f in os.listdir(labels_path) if f.endswith('.tif')])\n",
    "\n",
    "# def show_images(images,titles= None, cmap='gray'):\n",
    "#     n = len(images)\n",
    "#     fig, axes = plt.subplots(1, n, figsize=(20, 10))\n",
    "#     if not isinstance(axes, np.ndarray):\n",
    "#         axes = [axes]\n",
    "#     for idx, ax in enumerate(axes):\n",
    "#         ax.imshow(images[idx], cmap=cmap)\n",
    "#         if titles:\n",
    "#             ax.set_title(titles[idx])\n",
    "#         ax.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# first_image = tiff.imread(image_files[981])\n",
    "# first_label = tiff.imread(label_files[981])\n",
    "\n",
    "# show_images([first_image, first_label], titles=['First Image', 'First Label'])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files, \n",
    "                 input_size=(IMAGE_WIDTH, IMAGE_HEIGHT), \n",
    "                 augmentation_transforms=None):\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.input_size = input_size\n",
    "        self.augmentation_transforms = augmentation_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        image_path = self.image_files[idx]\n",
    "        mask_path = self.mask_files[idx]\n",
    "\n",
    "        image = preprocess_image(image_path)\n",
    "        mask = preprocess_mask(mask_path)\n",
    "\n",
    "        if self.augmentation_transforms:\n",
    "            image, mask = self.augmentation_transforms(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "class UsageDataset(Dataset):\n",
    "    def __init__(self, image_files, \n",
    "                 input_size=(IMAGE_WIDTH, IMAGE_HEIGHT), \n",
    "                 augmentation_transforms=None):\n",
    "        self.image_files = image_files\n",
    "        self.input_size = input_size\n",
    "        self.augmentation_transforms = augmentation_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        image_path = self.image_files[idx]\n",
    "\n",
    "        image, orig_size = preprocess_image(image_path, return_size=True)\n",
    "        # orig_size = image.shape\n",
    "\n",
    "        if self.augmentation_transforms:\n",
    "            image = self.augmentation_transforms(image)\n",
    "\n",
    "        return image, torch.tensor(np.array([orig_size[0], orig_size[1]]))\n",
    "\n",
    "\n",
    "class UsageDatasetTiled(Dataset):\n",
    "    def __init__(self, image_files, \n",
    "                 tile_size=(IMAGE_WIDTH, IMAGE_HEIGHT), \n",
    "                 augmentation_transforms=None):\n",
    "        \"\"\"\n",
    "        Warning, don't use resize in augmentations, it will mess up the tiles\n",
    "        \"\"\"\n",
    "        self.image_files = image_files\n",
    "        self.tile_size = tile_size\n",
    "        self.augmentation_transforms = augmentation_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        image_path = self.image_files[idx]\n",
    "\n",
    "        image, orig_size = preprocess_image(image_path, return_size=True)\n",
    "        # orig_size = image.shape\n",
    "\n",
    "        if self.augmentation_transforms:\n",
    "            image = self.augmentation_transforms(image)\n",
    "\n",
    "        return image, torch.tensor(np.array([orig_size[0], orig_size[1]]))\n",
    "\n",
    "\n",
    "def preprocess_image(path, return_size=False):\n",
    "    \n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    # img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    # print(f'fresh process image img.shape: {img.shape}')\n",
    "    img = np.tile(img[...,None],[1, 1, 3]) \n",
    "    img = img.astype('float32') \n",
    "\n",
    "    # scaling to 0-1\n",
    "    mx = np.max(img)\n",
    "    if mx:\n",
    "        img/=mx\n",
    "    \n",
    "    orig_size = img.shape\n",
    "    \n",
    "    # print(f'process image img.shape: {img.shape}')\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img_ten = torch.tensor(img)\n",
    "    if return_size:\n",
    "        return img_ten, orig_size\n",
    "    else:\n",
    "        return img_ten\n",
    "\n",
    "\n",
    "def preprocess_mask(path):\n",
    "    \n",
    "    msk = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    msk = msk.astype('float32')\n",
    "    msk/=255.0\n",
    "    msk_ten = torch.tensor(msk)\n",
    "    \n",
    "    return msk_ten\n",
    "\n",
    "def augment_image(image, mask):\n",
    "    \n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    mask_np = mask.numpy()\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.RandomCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, always_apply=True),\n",
    "        A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH, interpolation=cv2.INTER_NEAREST),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(scale_limit=(-0.1, 0.4), rotate_limit=15, shift_limit=0.1, p=0.8, border_mode=0),\n",
    "        A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Blur(blur_limit=3, p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.7,\n",
    "        ),\n",
    "        A.Emboss(alpha=HIGH_PASS_ALPHA, strength=HIGH_PASS_STRENGTH, always_apply=True),  # High pass filter\n",
    "    ])\n",
    "\n",
    "    augmented = transform(image=image_np, mask=mask_np)\n",
    "    augmented_image, augmented_mask = augmented['image'], augmented['mask']\n",
    "\n",
    "    augmented_image = torch.tensor(augmented_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "    augmented_mask = torch.tensor(augmented_mask, dtype=torch.float32)\n",
    "\n",
    "    return augmented_image, augmented_mask\n",
    "\n",
    "def val_transform(image, mask):\n",
    "    \n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    mask_np = mask.numpy()\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH, interpolation=cv2.INTER_NEAREST),\n",
    "        A.Emboss(alpha=HIGH_PASS_ALPHA, strength=HIGH_PASS_STRENGTH, always_apply=True),  # High pass filter\n",
    "    ])\n",
    "\n",
    "    augmented = transform(image=image_np, mask=mask_np)\n",
    "    augmented_image, augmented_mask = augmented['image'], augmented['mask']\n",
    "\n",
    "    augmented_image = torch.tensor(augmented_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "    augmented_mask = torch.tensor(augmented_mask, dtype=torch.float32)\n",
    "\n",
    "    return augmented_image, augmented_mask\n",
    "\n",
    "def create_loader(image_files, mask_files, batch_size, \n",
    "                  augmentations=None, shuffle=False):\n",
    "    \n",
    "    dataset = CustomDataset(image_files, mask_files, augmentation_transforms=augmentations)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def create_test_loader(image_files, batch_size, \n",
    "                  augmentations=None, shuffle=False):\n",
    "    \n",
    "    dataset = UsageDataset(image_files, augmentation_transforms=augmentations)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "\n",
    "#-------------------------- Test the dataset --------------------------#\n",
    "# image_files = []\n",
    "# label_files = []\n",
    "# for dataset in datasets:\n",
    "#     images_path = os.path.join(base_path, dataset, 'images')\n",
    "#     labels_path = os.path.join(base_path, dataset, 'labels')\n",
    "\n",
    "#     image_files.extend(sorted([os.path.join(images_path, f) for f in os.listdir(images_path) if f.endswith('.tif')]))\n",
    "#     label_files.extend(sorted([os.path.join(labels_path, f) for f in os.listdir(labels_path) if f.endswith('.tif')]))\n",
    "\n",
    "# # images_path = os.path.join(base_path, dataset, 'images')\n",
    "# # labels_path = os.path.join(base_path, dataset, 'labels')\n",
    "\n",
    "# # image_files = sorted([os.path.join(images_path, f) for f in os.listdir(images_path) if f.endswith('.tif')])\n",
    "# # label_files = sorted([os.path.join(labels_path, f) for f in os.listdir(labels_path) if f.endswith('.tif')])\n",
    "\n",
    "# train_image_files, val_image_files, train_mask_files, val_mask_files = train_test_split(\n",
    "#     image_files, label_files, test_size=0.1, random_state=42)\n",
    "\n",
    "# testing_path = 'data_downsampled512/train/test_output'\n",
    "# testing_img_files = sorted([os.path.join(testing_path, 'images', f) for f in os.listdir(testing_path+'/images') if f.endswith('.tif')])\n",
    "# testing_mask_files = sorted([os.path.join(testing_path, 'labels', f) for f in os.listdir(testing_path+'/labels') if f.endswith('.tif')])\n",
    "# # testing_mask_files = train_mask_files[:len(testing_img_files)]\n",
    "\n",
    "# train_dataset = CustomDataset(train_image_files, train_mask_files, augmentation_transforms=augment_image)\n",
    "# val_dataset = CustomDataset(val_image_files, val_mask_files, augmentation_transforms=val_transform)\n",
    "# test_of_dataset = CustomDataset(testing_img_files, testing_mask_files, augmentation_transforms=val_transform)\n",
    "\n",
    "# TRAIN_LOADER = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# VAL_LOADER = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# testing_loader = DataLoader(test_of_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# for batch_idx, (batch_images, batch_masks) in enumerate(VAL_LOADER):\n",
    "#     print(\"Batch\", batch_idx + 1)\n",
    "#     print(\"Image batch shape:\", batch_images.shape)\n",
    "#     print(\"Mask batch shape:\", batch_masks.shape)\n",
    "\n",
    "\n",
    "# for batch_idx, (batch_images, batch_masks) in enumerate(VAL_LOADER):\n",
    "#     print(\"Batch\", batch_idx + 1)\n",
    "#     print(\"Image batch shape:\", batch_images.shape)\n",
    "#     print(\"Mask batch shape:\", batch_masks.shape)\n",
    "    \n",
    "#     for image, mask, image_path, mask_path in zip(batch_images, batch_masks, train_image_files, train_mask_files):\n",
    "       \n",
    "#         image = image.permute((1, 2, 0)).numpy()*255.0\n",
    "#         image = image.astype('uint8')\n",
    "#         mask = (mask*255).numpy().astype('uint8')\n",
    "        \n",
    "#         image_filename = os.path.basename(image_path)\n",
    "#         mask_filename = os.path.basename(mask_path)\n",
    "        \n",
    "#         plt.figure(figsize=(15, 10))\n",
    "        \n",
    "#         plt.subplot(2, 4, 1)\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#         plt.title(f\"Original Image - {image_filename}\")\n",
    "        \n",
    "#         plt.subplot(2, 4, 2)\n",
    "#         plt.imshow(mask, cmap='gray')\n",
    "#         plt.title(f\"Mask Image - {mask_filename}\")\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#     break\n",
    "\n",
    "# for batch_idx, (batch_images, batch_masks) in enumerate(VAL_LOADER):\n",
    "#     print(\"Batch\", batch_idx + 1)\n",
    "#     print(\"Image batch shape:\", batch_images.shape)\n",
    "#     print(\"Mask batch shape:\", batch_masks.shape)\n",
    "    \n",
    "#     for image, mask, image_path, mask_path in zip(batch_images, batch_masks, train_image_files, train_mask_files):\n",
    "       \n",
    "#         image = image.permute((1, 2, 0)).numpy()*255.0\n",
    "#         image = image.astype('uint8')\n",
    "#         mask = (mask*255).numpy().astype('uint8')\n",
    "        \n",
    "#         image_filename = os.path.basename(image_path)\n",
    "#         mask_filename = os.path.basename(mask_path)\n",
    "        \n",
    "#         plt.figure(figsize=(15, 10))\n",
    "        \n",
    "#         plt.subplot(2, 4, 1)\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#         plt.title(f\"Original Image - {image_filename}\")\n",
    "        \n",
    "#         plt.subplot(2, 4, 2)\n",
    "#         plt.imshow(mask, cmap='gray')\n",
    "#         plt.title(f\"Mask Image - {mask_filename}\")\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A CNN model based on the Improved UNet architecture, with associated modules.\n",
    "\"\"\"\n",
    "# This is my 2D UNet from COMP3710\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ContextModule(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the context module from the improved UNet architecture.\n",
    "    \"Each context module is in fact a pre-activation residual block with two\n",
    "    3x3 convolutional layers and a dropout layer (pdrop = 0.3) in between.\"\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the ContextModule.\n",
    "        \"\"\"\n",
    "        super(ContextModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm1 = nn.InstanceNorm2d(out_channels)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=1e-2)\n",
    "        self.dropout = nn.Dropout2d(p=0.3)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm2d(out_channels)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=1e-2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ContextModule.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu2(out)\n",
    "        out += x # residual connection\n",
    "        return out\n",
    "\n",
    "class LocalisationModule(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the localization module from the improved UNet architecture.\n",
    "    A localization module consists of a 3x3 convolution followed by a 1x1 convolution that \n",
    "    halves the number of feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the LocalisationModule.\n",
    "        \"\"\"\n",
    "        super(LocalisationModule, self).__init__()\n",
    "        # self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the LocalisationModule.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsamplingModule(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling module consists of an upsampling layer that repeats the feature pixels \n",
    "    twice in each spatial dimension followed by a 3x3 convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the UpsamplingModule.\n",
    "        \"\"\"\n",
    "        super(UpsamplingModule, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the UpsamplingModule.\n",
    "        \"\"\"\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block with learnable parameters\"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_l, n_coefficients):\n",
    "        \"\"\"\n",
    "        :param F_g: number of feature maps (channels) in previous layer\n",
    "        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n",
    "        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n",
    "        \"\"\"\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.W_gate = nn.Sequential(\n",
    "            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, gate, skip_connection):\n",
    "        \"\"\"\n",
    "        :param gate: gating signal from previous layer\n",
    "        :param skip_connection: activation from corresponding encoder layer\n",
    "        :return: output activations\n",
    "        \"\"\"\n",
    "        g1 = self.W_gate(gate)\n",
    "        x1 = self.W_x(skip_connection)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        out = skip_connection * psi\n",
    "        return out\n",
    "\n",
    "\n",
    "class ImprovedUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the improved UNet model, it consists of the context aggregation pathway (encoder)\n",
    "    and the localization pathway (decoder). The model is designed to outpreform the original\n",
    "    UNet for medical image segmentation tasks.\n",
    "    See https://arxiv.org/pdf/1802.10508v1.pdf for network architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[16,32,64,128,256]):\n",
    "        \"\"\"\n",
    "        Initialize the ImprovedUNet model by creating all necessary layers.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int, optional): Number of input channels. Defaults to 3 for RGB color images.\n",
    "            out_channels (int, optional): Number of output channels. Defaults to 1 a for greyscale binary \n",
    "            segmentation mask.\n",
    "            features (list, optional): The numbers of feature maps to extract (must be length 5 and each \n",
    "            entry must be double the previous entry). Defaults to [16,32,64,128,256].\n",
    "        \"\"\"\n",
    "        super(ImprovedUNet, self).__init__()\n",
    "        self.encoder_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=1e-2),\n",
    "            ContextModule(features[0], features[0]), # 3 channels in, 16 channels out\n",
    "        )\n",
    "        self.encoder_block2 = nn.Sequential(\n",
    "            nn.Conv2d(features[0], features[1], kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=1e-2),\n",
    "            ContextModule(features[1], features[1]), # 16 channels in, 32 channels out\n",
    "        )\n",
    "        self.encoder_block3 = nn.Sequential(\n",
    "            nn.Conv2d(features[1], features[2], kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=1e-2),\n",
    "            ContextModule(features[2], features[2]), # 32 channels in, 64 channels out\n",
    "        )\n",
    "        self.encoder_block4 = nn.Sequential(\n",
    "            nn.Conv2d(features[2], features[3], kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=1e-2),\n",
    "            ContextModule(features[3], features[3]), # 64 channels in, 128 channels out\n",
    "        )\n",
    "        self.encoder_block5 = nn.Sequential(\n",
    "            nn.Conv2d(features[3], features[4], kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=1e-2),\n",
    "            ContextModule(features[4], features[4]), # 128 channels in, 256 channels out\n",
    "            # upsampling module in last encoder block increases spatial dimensions by 2 for decoder\n",
    "            UpsamplingModule(features[4], features[3]), # 256 channels in, 128 channels out\n",
    "        )\n",
    "        \n",
    "        # Adding in attention blocks gates at the concatenation of the skip connection and the bottleneck layer\n",
    "        self.attention1 = AttentionBlock(features[3], features[3], 32)\n",
    "        \n",
    "        # Upsampling modules half the number of feature maps but after upsampling, the output\n",
    "        # is concatenated with the skip connection, so the number of feature maps is doubled\n",
    "        # the localization modules then halve the number of feature maps again\n",
    "        self.decoder_block1 = nn.Sequential(\n",
    "            LocalisationModule(features[4], features[3]), # 256 channels in, 128 channels out\n",
    "            UpsamplingModule(features[3], features[2]), # 128 channels in, 64 channels out\n",
    "        )\n",
    "        self.attention2 = AttentionBlock(features[2], features[2], 64)\n",
    "        # these decoder layers need to be split up to allow for skip connections\n",
    "        self.localisation2 = LocalisationModule(features[3], features[2]) # 128 channels in, 64 channels out\n",
    "        self.up3 = UpsamplingModule(features[2], features[1]) # 64 channels in, 32 channels out, double spatial dimensions\n",
    "        self.attention3 = AttentionBlock(features[1], features[1], 128)\n",
    "        self.localisation3 = LocalisationModule(features[2], features[1]) # 64 channels in, 32 channels out\n",
    "        self.up4 = UpsamplingModule(features[1], features[0]) # 32 channels in, 16 channels out, double spatial dimensions\n",
    "        self.attention4 = AttentionBlock(features[0], features[0], 256)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(features[1], features[1], kernel_size=1), # 32 channels in, 32 channels out, final convolutional layer\n",
    "            nn.LeakyReLU(negative_slope=1e-2),\n",
    "            nn.Conv2d(features[1], out_channels, kernel_size=1) # 32 channels in, 1 channel out, segmentation layer\n",
    "        )\n",
    "        \n",
    "        self.segmentation1 = nn.Conv2d(features[2], out_channels, kernel_size=1) # 64 channels in, 1 channels out, segmentation layer\n",
    "        self.upscale1 = nn.Upsample(scale_factor=2, mode='nearest') # upscale the segmentation layer to match the dimensions of the output\n",
    "        self.segmentation2 = nn.Conv2d(features[1], out_channels, kernel_size=1) # 32 channels in, 1 channels out, segmentation layer\n",
    "        self.upscale2 = nn.Upsample(scale_factor=2, mode='nearest') # upscale the segmentation layer to match the dimensions of the output\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the ImprovedUNet model to generate a binary segmentation mask.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): An input image in tensor form.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary segmentation mask in tensor form\n",
    "        \"\"\"\n",
    "        skip_connections = []\n",
    "        \n",
    "        x = self.encoder_block1(x) # 3 channels in, 16 channels out\n",
    "        skip_connections.append(x)\n",
    "        x = self.encoder_block2(x) # 16 channels in, 32 channels out\n",
    "        skip_connections.append(x)\n",
    "        x = self.encoder_block3(x) # 32 channels in, 64 channels out\n",
    "        skip_connections.append(x)\n",
    "        x = self.encoder_block4(x) # 64 channels in, 128 channels out\n",
    "        skip_connections.append(x)\n",
    "        # bottleneck layer\n",
    "        x = self.encoder_block5(x) # 128 channels in, 128 channels out\n",
    "        \n",
    "        # use skip connections as a stack to allow for easy popping\n",
    "        # concatenate the skip connection with 128 channels with the bottleneck layer with 128 channels\n",
    "        # x = torch.cat((x, skip_connections.pop()), dim=1)\n",
    "        \n",
    "        # First attention block\n",
    "        a = self.attention1(x, skip_connections.pop())\n",
    "        x = torch.cat((a, x), dim=1)\n",
    "        \n",
    "        x = self.decoder_block1(x) # 256 channels in, 64 channels out\n",
    "        # concatenate the skip connection with 64 channels with the bottleneck layer with 64 channels\n",
    "        # x = torch.cat((x, skip_connections.pop()), dim=1)\n",
    "        \n",
    "        # Second attention block\n",
    "        a = self.attention2(x, skip_connections.pop())\n",
    "        x = torch.cat((a, x), dim=1)\n",
    "        \n",
    "        x = self.localisation2(x) # 128 channels in, 64 channels out\n",
    "        # additional skip connections for segmentation layers in the decoder\n",
    "        seg_connection1 = self.segmentation1(x) # 64 channels in, 1 channel out\n",
    "        seg_connection1 = self.upscale1(seg_connection1)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        \n",
    "        # concatenate the skip connection with 32 channels with the bottleneck layer with 32 channels\n",
    "        # x = torch.cat((x, skip_connections.pop()), dim=1)\n",
    "        \n",
    "        # Third attention block\n",
    "        a = self.attention3(x, skip_connections.pop())\n",
    "        x = torch.cat((a, x), dim=1)\n",
    "        \n",
    "        x = self.localisation3(x) # 64 channels in, 32 channels out\n",
    "        # additional skip connections for segmentation layers in the decoder\n",
    "        seg_connection2 = self.segmentation2(x) # 32 channels in, 1 channel out\n",
    "        # element wise addition of the segmentation connections\n",
    "        seg_connection2 += seg_connection1\n",
    "        seg_connection2 = self.upscale2(seg_connection2) # upscale the segmentation connection to match the dimensions of the output\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        # concatenate the skip connection with 16 channels with the bottleneck layer with 16 channels\n",
    "        # x = torch.cat((x, skip_connections.pop()), dim=1)\n",
    "        \n",
    "        # Fourth attention block\n",
    "        a = self.attention4(x, skip_connections.pop())\n",
    "        x = torch.cat((a, x), dim=1)\n",
    "        \n",
    "        x = self.final_conv(x) # 32 channels in, 1 channel out, includes final convolutional layer and segmentation layer\n",
    "        \n",
    "        # element wise addition of the segmentation connections\n",
    "        x += seg_connection2\n",
    "        \n",
    "        # use sigmoid activation function to squash the output to between 0 and 1\n",
    "        # x = torch.sigmoid(x)\n",
    "        return x\n",
    "        # return x # no final activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoints/checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Saves the model and optimizer state dicts to a checkpoint file\n",
    "    \"\"\"\n",
    "    print('>>> Saving checkpoint')\n",
    "    # os.makedirs('checkpoints', exist_ok=True)\n",
    "    torch.save(state, filename)\n",
    "    print('>>> Checkpoint saved')\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"\n",
    "    Loads the model and optimizer state dicts from a checkpoint file\n",
    "    \"\"\"\n",
    "    print('>>> Loading checkpoint')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print('>>> Checkpoint loaded')\n",
    "\n",
    "def remove_small_objects(img, min_size):\n",
    "    # Find all connected components (labels)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n",
    "\n",
    "    # Create a mask where small objects are removed\n",
    "    new_img = np.zeros_like(img)\n",
    "    for label in range(1, num_labels):\n",
    "        if stats[label, cv2.CC_STAT_AREA] >= min_size:\n",
    "            new_img[labels == label] = 1\n",
    "\n",
    "    return new_img\n",
    "\n",
    "def rle_encode(mask, bg = 0) -> dict:\n",
    "    vec = mask.flatten()\n",
    "    nb = len(vec)\n",
    "    where = np.flatnonzero\n",
    "    starts = np.r_[0, where(~np.isclose(vec[1:], vec[:-1], equal_nan=True)) + 1]\n",
    "    lengths = np.diff(np.r_[starts, nb])\n",
    "    values = vec[starts]\n",
    "    assert len(starts) == len(lengths) == len(values)\n",
    "    rle = []\n",
    "    for start, length, val in zip(starts, lengths, values):\n",
    "        if val == bg:\n",
    "            continue\n",
    "        rle += [str(start), length]\n",
    "    # post-processing\n",
    "    return \" \".join(map(str, rle))\n",
    "\n",
    "def create_rle_df(kidney_dirs: [str], \n",
    "                  subdir_name: str = 'preds',\n",
    "                  preds_path=None,\n",
    "                  img_size=(512, 512),\n",
    "                  resize=True,\n",
    "                  orig_path='/kaggle/input/blood-vessel-segmentation/test/'):\n",
    "    \"\"\"\n",
    "    Creates a dataframe with the image ids and the predicted masks\n",
    "    \"\"\"\n",
    "    if type(kidney_dirs) == str:\n",
    "        kidney_dirs = [kidney_dirs]\n",
    "    \n",
    "    df = pd.DataFrame(columns=['id', 'rle'])\n",
    "    # df.set_index('id', inplace=True)\n",
    "        \n",
    "    for kidney_dir in kidney_dirs:\n",
    "        assert os.path.exists(kidney_dir), f'{kidney_dir} does not exist'\n",
    "        \n",
    "        kidney_name = os.path.basename(kidney_dir)\n",
    "        print(f'KIDNEY NAME: {kidney_name}')\n",
    "        \n",
    "        if resize:\n",
    "            example_path = os.path.join(orig_path, kidney_name, 'images', '0000.tif')\n",
    "            assert os.path.exists(example_path), f'{example_path} does not exist'\n",
    "            orig_size = cv2.imread(example_path).shape[:2]\n",
    "            print(f'KIDNEY IMG SIZE: {orig_size}')\n",
    "            img_size = (orig_size[1], orig_size[0])\n",
    "        \n",
    "        if preds_path == None:\n",
    "            masks = sorted([os.path.join(kidney_dir, subdir_name, f) for f in \n",
    "                            os.listdir(os.path.join(kidney_dir, subdir_name)) if f.endswith('.tif')])\n",
    "        else:\n",
    "            masks = sorted([os.path.join(preds_path, f) for f in \n",
    "                            os.listdir(os.path.join(preds_path)) if f.endswith('.tif')])\n",
    "        \n",
    "        for mask_file in masks:\n",
    "            mask_name = os.path.basename(mask_file)\n",
    "            mask = Image.open(mask_file).convert('L').resize(img_size, resample=Image.BOX)\n",
    "            \n",
    "#             plt.imshow(mask)\n",
    "            \n",
    "            mask = np.array(mask, dtype=np.float32)\n",
    "#             mask[mask==1] = 255\n",
    "            mask = remove_small_objects(mask, 10) # added this to try and fix scoring error \n",
    "\n",
    "#             print(f'MASK SHAPE: {mask.shape}, MASK MAX: {mask.max()}')\n",
    "            \n",
    "            \n",
    "            id = f'{kidney_name}_{mask_name[:-4]}'\n",
    "            rle = rle_encode(mask)\n",
    "#             print(f'ID --------------> {id}')\n",
    "#             print(f'RLE -------------> {rle}')\n",
    "#             print(f'RLE TYPE: {type(rle)}')\n",
    "#             print('------------ END OF ENTRY ------------')\n",
    "            if rle == '':\n",
    "                rle = '1 0'\n",
    "            # df.loc[id] = rle\n",
    "            df.loc[len(df)] = [id, rle]\n",
    "        \n",
    "    # =========== MIGHT WANT TO REMOVE THIS ===============\n",
    "#     df['width'] = [img_size[0]] * len(df)\n",
    "#     df['height'] = [img_size[1]] * len(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_predictions(kidney_dirs, model, num='all', min_idx=0,\n",
    "                     folder='saved_images/', device='cuda', \n",
    "                     verbose=True, image_size=(IMAGE_HEIGHT, IMAGE_WIDTH)):\n",
    "    \"\"\"\n",
    "    Saves the predictions from the model as images in the folder\n",
    "    \"\"\"\n",
    "    if type(kidney_dirs) == str:\n",
    "        kidney_dirs = [kidney_dirs]\n",
    "\n",
    "    for kidney_dir in kidney_dirs:\n",
    "        assert os.path.exists(kidney_dir), f'{kidney_dir} does not exist'\n",
    "        \n",
    "        kidney_name = os.path.basename(kidney_dir)\n",
    "        \n",
    "        preds_path = os.path.join(folder, kidney_name, 'preds')\n",
    "#         masks_path = os.path.join(folder, kidney_name, 'labels')\n",
    "        images_path = os.path.join(folder, kidney_name, 'images')\n",
    "        os.makedirs(preds_path, exist_ok=True)\n",
    "#         os.makedirs(masks_path, exist_ok=True)\n",
    "        os.makedirs(images_path, exist_ok=True)\n",
    "        \n",
    "#         masks = sorted([os.path.join(kidney_dir, 'labels', f) for f in \n",
    "#                         os.listdir(os.path.join(kidney_dir, 'labels')) if f.endswith('.tif')])\n",
    "        images = sorted([os.path.join(kidney_dir, 'images', f) for f in \n",
    "                        os.listdir(os.path.join(kidney_dir, 'images')) if f.endswith('.tif')])\n",
    "                \n",
    "        if num == 'all':\n",
    "            num = len(images)\n",
    "        else:\n",
    "            num = min(num, len(images))\n",
    "        \n",
    "        assert min_idx + num <= len(images), f'Index out of range. There are {len(images)} images.'\n",
    "        \n",
    "        # transforms = A.Compose([\n",
    "        #     A.Resize(image_size[0], image_size[1], interpolation=cv2.INTER_NEAREST),\n",
    "        #     A.Emboss(alpha=HIGH_PASS_ALPHA, strength=HIGH_PASS_STRENGTH, always_apply=True),  # High pass filter\n",
    "        #     ToTensorV2()\n",
    "        # ])\n",
    "        loader = create_loader(images, images, batch_size=1, augmentations=val_transform)\n",
    "        model.eval()\n",
    "        print('>>> Generating and saving predictions') if verbose else None\n",
    "        loop = tqdm(enumerate(loader), total=num, leave=False) if verbose else enumerate(loader)\n",
    "        with torch.no_grad():\n",
    "            for idx, (x, y) in loop:\n",
    "                if idx < min_idx:\n",
    "                    continue\n",
    "                \n",
    "                filename = os.path.basename(images[idx])\n",
    "                x = x.to(device)\n",
    "                y = y.to(device) # add 1 channel to mask\n",
    "                preds = torch.sigmoid(model(x))\n",
    "                preds = (preds > PREDICTION_THRESHOLD).float()\n",
    "                torchvision.utils.save_image(preds, os.path.join(preds_path, filename))\n",
    "#                 torchvision.utils.save_image(y.unsqueeze(1), os.path.join(masks_path, filename))\n",
    "                torchvision.utils.save_image(x, os.path.join(images_path, filename))\n",
    "                if idx > min_idx + num:\n",
    "                    break\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform(image):\n",
    "    \n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH, interpolation=cv2.INTER_NEAREST),\n",
    "        A.Emboss(alpha=HIGH_PASS_ALPHA, strength=HIGH_PASS_STRENGTH, always_apply=True),  # High pass filter\n",
    "    ])\n",
    "\n",
    "    augmented = transform(image=image_np)\n",
    "    augmented_image = augmented['image']\n",
    "\n",
    "    augmented_image = torch.tensor(augmented_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "    return augmented_image\n",
    "\n",
    "def inference(model, device, dataset_folder=\"/kaggle/input/blood-vessel-segmentation\", is_submission=True):\n",
    "    # dataset_folder\n",
    "    if is_submission:\n",
    "        ls_images = glob(os.path.join(dataset_folder, \"test\", \"*\", \"*\", \"*.tif\"))\n",
    "    else:\n",
    "        ls_images = glob(os.path.join(VAL_DATASET_DIR, \"images\", \"*.tif\"))\n",
    "        ls_images = ls_images[DATA_SUBSECTION[0]:DATA_SUBSECTION[1]] ###########################\n",
    "    test_loader = create_test_loader(ls_images, BATCH_SIZE, augmentations=test_transform)\n",
    "\n",
    "    rles = []\n",
    "    widths = []\n",
    "    heights = []\n",
    "    pbar = tqdm(enumerate(test_loader), total=len(test_loader), desc='Inference ')\n",
    "    for step, (images, shapes) in pbar:\n",
    "        shapes = np.array(shapes)\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(images)\n",
    "            preds = (nn.Sigmoid()(preds)>PREDICTION_THRESHOLD).double()\n",
    "        preds = preds.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        for pred, shape in zip(preds, shapes):\n",
    "            pred = cv2.resize(pred[0], (shape[1], shape[0]), cv2.INTER_NEAREST)\n",
    "            pred = remove_small_objects(pred, 10)\n",
    "            if IS_LOCAL:\n",
    "                # plt.imshow(pred)\n",
    "                # plt.show()\n",
    "                pass\n",
    "            rle = rle_encode(pred)\n",
    "            if rle == '':\n",
    "                rle = '1 0'\n",
    "\n",
    "            widths.append(shape[1])\n",
    "            heights.append(shape[0])\n",
    "            rles.append(rle)\n",
    "\n",
    "    ids = []\n",
    "    for p_img in tqdm(ls_images):\n",
    "        path_ = p_img.split(os.path.sep)\n",
    "        # parse the submission ID\n",
    "        dataset = path_[-3]\n",
    "        slice_id, _ = os.path.splitext(path_[-1])\n",
    "        ids.append(f\"{dataset}_{slice_id}\")\n",
    "    \n",
    "    if IS_SUBMISSION:\n",
    "        submission = pd.DataFrame.from_dict({\n",
    "            \"id\": ids,\n",
    "            \"rle\": rles\n",
    "        })\n",
    "    else:\n",
    "        assert len(ids) == len(rles) == len(widths) == len(heights), \\\n",
    "            f'Length of ids, rles, widths, heights do not match. ids: \\\n",
    "              {len(ids)}, rles: {len(rles)}, widths: {len(widths)}, heights: {len(heights)}'\n",
    "        submission = pd.DataFrame.from_dict({\n",
    "            \"id\": ids,\n",
    "            \"rle\": rles,\n",
    "            \"width\": widths,\n",
    "            \"height\": heights\n",
    "        })\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import fast_compute_surface_dice_score_from_tensor\n",
    "\n",
    "def local_surface_dice(model, device, dataset_folder=\"/data/train/kidney_2\", sub_data_idxs=None):\n",
    "    \n",
    "    ls_images = glob(os.path.join(dataset_folder, \"images\", \"*.tif\"))\n",
    "    ls_masks = glob(os.path.join(dataset_folder, 'labels', \"*.tif\"))\n",
    "    if sub_data_idxs:\n",
    "        ls_images = ls_images[sub_data_idxs[0]:sub_data_idxs[1]]\n",
    "        ls_masks = ls_masks[sub_data_idxs[0]:sub_data_idxs[1]]\n",
    "    test_loader = create_test_loader(ls_images, BATCH_SIZE, augmentations=test_transform)\n",
    "\n",
    "    surface_dice_scores = []\n",
    "    pbar = tqdm(enumerate(test_loader), total=len(test_loader), desc='Inference ')\n",
    "    for batch_idx, (images, shapes) in pbar:\n",
    "        shapes = np.array(shapes)\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(images)\n",
    "            preds = (nn.Sigmoid()(preds)>PREDICTION_THRESHOLD).double()\n",
    "        preds = preds.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        for i, (pred, shape) in enumerate(zip(preds, shapes)):\n",
    "            pred = cv2.resize(pred[0], (shape[1], shape[0]), cv2.INTER_NEAREST)\n",
    "            # pred = cv2.resize(pred[0], (shape[1], shape[0]), cv2.INTER_CUBIC)\n",
    "             \n",
    "            # pred = remove_small_objects(pred, 10)\n",
    "            true_mask = preprocess_mask(ls_masks[batch_idx*BATCH_SIZE+i])\n",
    "\n",
    "            # The dimensions of the true mask and pred are [width, height] but need to be [1, width, height]\n",
    "            # they need to stay as numpy arrays not torch tensors\n",
    "            pred = np.expand_dims(pred, axis=0)\n",
    "            true_mask = np.expand_dims(true_mask, axis=0)\n",
    "\n",
    "            # print(f'pred shape: {pred.shape}, true_mask shape: {true_mask.shape}')\n",
    "\n",
    "            # compute surface dice score\n",
    "            surface_dice_score = fast_compute_surface_dice_score_from_tensor(pred, true_mask)\n",
    "            surface_dice_scores.append(surface_dice_score)\n",
    "    \n",
    "    mean_surface_dice_score = np.mean(surface_dice_scores)\n",
    "    return mean_surface_dice_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading checkpoint\n",
      ">>> Checkpoint loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference : 100%|██████████| 30/30 [00:35<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface Dice Score: 0.9055052082593567\n",
      "INFERENCE SUCCESSFULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Coppied from https://www.kaggle.com/code/kashiwaba/sennet-hoa-inference-unet-simple-baseline\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tesd_dir = '/kaggle/input/blood-vessel-segmentation/test/'\n",
    "\n",
    "def main():\n",
    "    model = ImprovedUNet(in_channels=3, out_channels=1).to(device=device)\n",
    "    load_checkpoint(torch.load(CHECKPOINT_DIR), model)\n",
    "    \n",
    "    kidney_paths = glob(os.path.join(tesd_dir, 'kidney_*'))\n",
    "    \n",
    "    try:\n",
    "        save_predictions(kidney_paths, model, folder='', device=device)\n",
    "    except:\n",
    "        print('failure in saving and generating predictions')\n",
    "\n",
    "    kidney_names = [k.split('/')[-1] for k in kidney_paths]\n",
    "    \n",
    "    if IS_LOCAL:\n",
    "        # prediction_rles = inference(model, device, dataset_folder='../../data', is_submission=IS_SUBMISSION)\n",
    "        surface_dice = local_surface_dice(model, device, dataset_folder=VAL_DATASET_DIR, sub_data_idxs=DATA_SUBSECTION)\n",
    "        print(f'Surface Dice Score: {surface_dice}')\n",
    "    else:\n",
    "        prediction_rles = inference(model, device, is_submission=IS_SUBMISSION)\n",
    "\n",
    "    # Deleting the temporary files\n",
    "    for k in kidney_names:\n",
    "        shutil.rmtree(k, ignore_errors=True)\n",
    "\n",
    "    if IS_SUBMISSION:\n",
    "        prediction_rles.to_csv('submission.csv', index=False)\n",
    "    # elif IS_LOCAL:\n",
    "        # from surface_dice import score\n",
    "        # true_msk_paths = glob(os.path.join(VAL_DATASET_DIR, 'labels', '*.tif'))\n",
    "        # true_msk_paths = true_msk_paths[DATA_SUBSECTION[0]:DATA_SUBSECTION[1]] ###########################\n",
    "        # true_msk_paths.sort()\n",
    "        # ids = []\n",
    "        # rles = []\n",
    "        # widths = []\n",
    "        # heights = []\n",
    "        # for p in tqdm(true_msk_paths):\n",
    "        #     msk = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        #     msk = np.where(msk>0, 1, 0).astype(np.uint8)\n",
    "        #     rle = rle_encode(msk)\n",
    "        #     if rle == '':\n",
    "        #         rle = '1 0'\n",
    "        #     if type(rle) != str:\n",
    "        #         print(f'RLE NOT STRINT! ID: {os.path.basename(p)}, RLE: {rle}')\n",
    "        #     rles.append(rle)\n",
    "        #     ids.append(os.path.splitext(os.path.basename(p)))\n",
    "        #     widths.append(msk.shape[1])\n",
    "        #     heights.append(msk.shape[0])\n",
    "            \n",
    "        # assert len(ids) == len(rles) == len(widths) == len(heights), \\\n",
    "        # f'Length of ids, rles, widths, heights do not match. ids: \\\n",
    "        #     {len(ids)}, rles: {len(rles)}, widths: {len(widths)}, heights: {len(heights)}'\n",
    "        # true_df = pd.DataFrame.from_dict({\n",
    "        #     'id': ids,\n",
    "        #     'rle': rles,\n",
    "        #     'width': widths,\n",
    "        #     'height': heights\n",
    "        # })\n",
    "\n",
    "        # surface_dice_score = score(true_df, prediction_rles, 'id', 'rle')\n",
    "        # print(f'Surface Dice Score: {surface_dice_score}')\n",
    "\n",
    "\n",
    "    \n",
    "    print('INFERENCE SUCCESSFULL')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3710-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
